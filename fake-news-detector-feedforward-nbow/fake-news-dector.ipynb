{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnnwupedFEIV"
      },
      "source": [
        "### Part 0. Google Colab Setup"
      ],
      "id": "JnnwupedFEIV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiFjTT4jFEIW"
      },
      "source": [
        "Welcome to the first full programming project for CS 4650! If you're new to Google Colab we recommend looking at [this](https://colab.research.google.com/notebooks/basic_features_overview.ipynb) intro notebook before getting started with this project. In short, Colab is a Jupyter notebook environment that runs in the cloud, it's recommended for all of the programming projects in this course due to its availability, ease of use, and hardware accessibility. Some features that you may find especially useful are on the left hand side, these being:\n",
        "\n",
        "*   Table of contents: displays the sections of the notebook made using text cells\n",
        "*   Variables: useful for debugging and see current values of variables\n",
        "*   Files: useful or uploading or downloading any files you upload to Colab or write while working on the projects\n",
        "\n",
        "\n",
        "\n",
        "**To begin this project, make a copy of this notebook and save it to your local drive so that you can edit it.**\n",
        "\n",
        "\n",
        "If you want GPU's (which will improve training times), you can always change your instance type to GPU by going to Runtime -> Change runtime type -> Hardware accelerator.\n",
        "\n",
        "If you're new to PyTorch, or simply want a refresher, we recommend you start by looking through these [Introduction to PyTorch](https://cocoxu.github.io/CS4650_spring2022/slides/PyTorch_tutorial.pdf) slides and this interactive [PyTorch Basics notebook](http://bit.ly/pytorchbasics). Additionally, this [Text Sentiment](http://bit.ly/pytorchexample) notebook will provide some insight into working with PyTorch for NLP specific problems. "
      ],
      "id": "SiFjTT4jFEIW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRdPqeMMFEIY"
      },
      "source": [
        "### Part 1. Loading and Preprocessing Data [10 points]\n",
        "The following cell loads the OnionOrNot dataset, and tokenizes each data item"
      ],
      "id": "ZRdPqeMMFEIY"
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/lukefeilberg/onion/master/OnionOrNot.csv > OnionOrNot.csv"
      ],
      "metadata": {
        "id": "YmV_uknBJA-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "075e4026-2448-4a3e-bc07-c5e18aaed7a2"
      },
      "id": "YmV_uknBJA-o",
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1903k  100 1903k    0     0  2742k      0 --:--:-- --:--:-- --:--:-- 2738k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "L3DkMDu7FEIZ"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY #\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# this is how we select a GPU if it's avalible on your computer or in the Colab environment.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "id": "L3DkMDu7FEIZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1.1 Preprocessing definitions:\n",
        "The following cell define some methods to clean the dataset. Do not edit it, but feel free to take a look at some of the operations it's doing. \n"
      ],
      "metadata": {
        "id": "0Fulh0MZ8y8b"
      },
      "id": "0Fulh0MZ8y8b"
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS BLOCK\n",
        "# example code taken from fast-bert\n",
        "\n",
        "import re\n",
        "import html\n",
        "\n",
        "def spec_add_spaces(t: str) -> str:\n",
        "    \"Add spaces around / and # in `t`. \\n\"\n",
        "    return re.sub(r\"([/#\\n])\", r\" \\1 \", t)\n",
        "\n",
        "def rm_useless_spaces(t: str) -> str:\n",
        "    \"Remove multiple spaces in `t`.\"\n",
        "    return re.sub(\" {2,}\", \" \", t)\n",
        "\n",
        "def replace_multi_newline(t: str) -> str:\n",
        "    return re.sub(r\"(\\n(\\s)*){2,}\", \"\\n\", t)\n",
        "\n",
        "def fix_html(x: str) -> str:\n",
        "    \"List of replacements from html strings in `x`.\"\n",
        "    re1 = re.compile(r\"  +\")\n",
        "    x = (\n",
        "        x.replace(\"#39;\", \"'\")\n",
        "        .replace(\"amp;\", \"&\")\n",
        "        .replace(\"#146;\", \"'\")\n",
        "        .replace(\"nbsp;\", \" \")\n",
        "        .replace(\"#36;\", \"$\")\n",
        "        .replace(\"\\\\n\", \"\\n\")\n",
        "        .replace(\"quot;\", \"'\")\n",
        "        .replace(\"<br />\", \"\\n\")\n",
        "        .replace('\\\\\"', '\"')\n",
        "        .replace(\" @.@ \", \".\")\n",
        "        .replace(\" @-@ \", \"-\")\n",
        "        .replace(\" @,@ \", \",\")\n",
        "        .replace(\"\\\\\", \" \\\\ \")\n",
        "    )\n",
        "    return re1.sub(\" \", html.unescape(x))\n",
        "\n",
        "def clean_text(input_text):\n",
        "    text = fix_html(input_text)\n",
        "    text = replace_multi_newline(text)\n",
        "    text = spec_add_spaces(text)\n",
        "    text = rm_useless_spaces(text)\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "ctNnE1Ui8oKw"
      },
      "id": "ctNnE1Ui8oKw",
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1.2 Clean the data using the methods above and tokenize it using NLTK"
      ],
      "metadata": {
        "id": "MiUlTSBB9Wx6"
      },
      "id": "MiUlTSBB9Wx6"
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "vqtdrhF8FEIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea99d749-d295-45ca-f019-6fb12b5fd3db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')\n",
        "# read data from the csv file\n",
        "df              = pd.read_csv(\"OnionOrNot.csv\")\n",
        "# tokenizing text data -> nltk.word_tokenize() deals with tokening and clean_text prunes the string data\n",
        "df[\"tokenized\"] = df[\"text\"].apply(lambda x: nltk.word_tokenize(clean_text(x.lower())))\n"
      ],
      "id": "vqtdrhF8FEIZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBBdVOYxFEIa"
      },
      "source": [
        "Here's what the dataset looks like. You can index into specific rows with pandas, and try to guess some of these yourself :). If you're unfamiliar with pandas, it's a extremely useful and popular library for data analysis and manipulation. You can find their documentation [here](https://pandas.pydata.org/docs/). \n",
        "\n",
        "Pandas primary data structure is a DataFrame. The following cell will print out the basic information of this structure, including the labeled axes (both columns and rows) as well as show you what the first n (default=5) rows look like"
      ],
      "id": "qBBdVOYxFEIa"
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "sJjScqV3FEIb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ea044bb9-d220-4c79-d18e-766484ec6f39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7d1abd02-0459-41fa-8877-e17cd52ecdd7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Entire Facebook Staff Laughs As Man Tightens P...</td>\n",
              "      <td>1</td>\n",
              "      <td>[entire, facebook, staff, laughs, as, man, tig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Muslim Woman Denied Soda Can for Fear She Coul...</td>\n",
              "      <td>0</td>\n",
              "      <td>[muslim, woman, denied, soda, can, for, fear, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bold Move: Hulu Has Announced That They’re Gon...</td>\n",
              "      <td>1</td>\n",
              "      <td>[bold, move, :, hulu, has, announced, that, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despondent Jeff Bezos Realizes He’ll Have To W...</td>\n",
              "      <td>1</td>\n",
              "      <td>[despondent, jeff, bezos, realizes, he, ’, ll,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For men looking for great single women, online...</td>\n",
              "      <td>1</td>\n",
              "      <td>[for, men, looking, for, great, single, women,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d1abd02-0459-41fa-8877-e17cd52ecdd7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7d1abd02-0459-41fa-8877-e17cd52ecdd7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7d1abd02-0459-41fa-8877-e17cd52ecdd7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text  ...                                          tokenized\n",
              "0  Entire Facebook Staff Laughs As Man Tightens P...  ...  [entire, facebook, staff, laughs, as, man, tig...\n",
              "1  Muslim Woman Denied Soda Can for Fear She Coul...  ...  [muslim, woman, denied, soda, can, for, fear, ...\n",
              "2  Bold Move: Hulu Has Announced That They’re Gon...  ...  [bold, move, :, hulu, has, announced, that, th...\n",
              "3  Despondent Jeff Bezos Realizes He’ll Have To W...  ...  [despondent, jeff, bezos, realizes, he, ’, ll,...\n",
              "4  For men looking for great single women, online...  ...  [for, men, looking, for, great, single, women,...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ],
      "source": [
        "df.head()"
      ],
      "id": "sJjScqV3FEIb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrames can be indexed using [.iloc\\[ ]](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html), this primarily uses interger based indexing and supports a single integer (i.e. 42), a list of integers (i.e. [1, 5, 42]), or even a slice (i.e. 7:42). "
      ],
      "metadata": {
        "id": "D9b4W9z1XhgS"
      },
      "id": "D9b4W9z1XhgS"
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "Ntm8laX6FEIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1efe774d-2f2b-45c3-f8d4-24571d448d77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text         Customers continued to wait at drive-thru even...\n",
              "label                                                        0\n",
              "tokenized    [customers, continued, to, wait, at, drive-thr...\n",
              "Name: 42, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ],
      "source": [
        "df.iloc[42]\n",
        "\n"
      ],
      "id": "Ntm8laX6FEIb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1.3 Split the dataset into training, validation, and testing"
      ],
      "metadata": {
        "id": "TQVT6HUA9htQ"
      },
      "id": "TQVT6HUA9htQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDI72x8XFEIc"
      },
      "source": [
        "Now that we've loaded this dataset, we need to split the data into train, validation, and test sets. A good explanation of why we need these different sets can be found in subsection 2.2.5 of [Eisenstein](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) but at the end it comes down to having a trustworthy and generalized model. The validation set (sometimes called a development or tuning) is used to help choose hyperparameters for our model, whereas the training set is used to fit the learned parameters (weights and biases) to the task. The test set is used to provide a final unbiased evaluation of our trained model, hopefully providing some insight into how it would actually do in production. Each of these sets should be disjoint from the others, to prevent any \"peeking\" that could unfairly influence our understanding of the model's accuracy. \n",
        "\n",
        "In addition to these different sets of data, we also need to create a vocab map for words in our Onion dataset, which will map tokens to numbers. This will be useful later, since torch models can only use tensors of sequences of numbers as inputs. **Go to the following cell, and fill out split_train_val_test and generate_vocab_map.**"
      ],
      "id": "GDI72x8XFEIc"
    },
    {
      "cell_type": "code",
      "source": [
        "# BEGIN - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "from collections import Counter\n",
        "PADDING_VALUE = 0\n",
        "UNK_VALUE     = 1\n",
        "# END - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "\n",
        "\n",
        "# split_train_val_test\n",
        "# This method takes a dataframe and splits it into train/val/test splits.\n",
        "# It uses the props argument to split the dataset appropriately.\n",
        "#\n",
        "# args:\n",
        "# df - the entire dataset DataFrame \n",
        "# props - proportions for each split. the last value of the props array \n",
        "#         is repetitive, but we've kept it for clarity.\n",
        "#\n",
        "# returns: \n",
        "# train DataFrame, val DataFrame, test DataFrame\n",
        "#\n",
        "def split_train_val_test(df, props=[.8, .1, .1]):\n",
        "    assert round(sum(props), 2) == 1 and len(props) >= 2\n",
        "    train_df, test_df, val_df = None, None, None\n",
        "    \n",
        "    ## YOUR CODE STARTS HERE (~3-5 lines of code) ##\n",
        "    # hint: you can use df.iloc to slice into specific indexes or ranges.\n",
        "    df_size = len(df)\n",
        "    train_bound, val_bound, test_bound = int(props[0]*df_size), int((props[0] + props[1])*df_size), int((props[0] + props[1] + props[2])*df_size)\n",
        "\n",
        "    train_df, val_df, test_df = df.iloc[: train_bound], df.iloc[train_bound : val_bound], df.iloc[val_bound : test_bound]\n",
        "  \n",
        "    \n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    \n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# generate_vocab_map\n",
        "# This method takes a dataframe and builds a vocabulary to unique number map.\n",
        "# It uses the cutoff argument to remove rare words occuring <= cutoff times. \n",
        "# *NOTE*: \"\" and \"UNK\" are reserved tokens in our vocab that will be useful\n",
        "# later.\n",
        "# \n",
        "# args:\n",
        "# df - the entire dataset DataFrame \n",
        "# cutoff - we exclude words from the vocab that appear less than or\n",
        "#          eq to cutoff\n",
        "#\n",
        "# returns: \n",
        "# vocab - dict[str] = int\n",
        "#         In vocab, each str is a unique token, and each dict[str] is a \n",
        "#         unique integer ID. Only elements that appear > cutoff times appear\n",
        "#         in vocab.\n",
        "#\n",
        "# reversed_vocab - dict[int] = str\n",
        "#                  A reversed version of vocab, which allows us to retrieve \n",
        "#                  words given their unique integer ID. This map will \n",
        "#                  allow us to \"decode\" integer sequences we'll encode using\n",
        "#                  vocab!\n",
        "# \n",
        "def generate_vocab_map(df, cutoff=2):\n",
        "    vocab          = {\"\": PADDING_VALUE, \"UNK\": UNK_VALUE}\n",
        "    reversed_vocab = None\n",
        "    \n",
        "    ## YOUR CODE STARTS HERE (~5-15 lines of code) ##\n",
        "    # hint: start by iterating over df[\"tokenized\"]\n",
        "\n",
        "    # compute how much each word appears in the entire dataset given\n",
        "    df_counter = Counter()\n",
        "    for sample in df[\"tokenized\"]:\n",
        "        df_counter.update(sample)\n",
        "\n",
        "    # build 'vocab' and 'reversed_vocab'\n",
        "    reversed_vocab = {UNK_VALUE: \"UNK\", PADDING_VALUE: \"\"} # 0, 1\n",
        "    \n",
        "    # word_id starts from 2 (\"vocab\" already has 2 elements in it (PADDING_VALUE & UNK_VALUE))\n",
        "    word_id = 2\n",
        "    for word in df_counter:\n",
        "        if (df_counter[word] > cutoff) and (word not in vocab):\n",
        "            vocab[word] = word_id\n",
        "            reversed_vocab[word_id] = word\n",
        "            word_id += 1\n",
        "\n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    \n",
        "    return vocab, reversed_vocab"
      ],
      "metadata": {
        "id": "zeo9kX6i9pbH"
      },
      "id": "zeo9kX6i9pbH",
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "rcmX931OFEId"
      },
      "outputs": [],
      "source": [
        "# With the methods you have implemented above, we can now split the dataset into training, validation, and testing\n",
        "#   sets and generate our dictionaries mapping from word tokens to IDs (and vice versa).\n",
        "# Note: The props list currently being used splits the dataset so that 80% of samples are used to traing, and the \n",
        "#   remaining 20% are evenly split between training and validation. How you split your dataset is itself a major\n",
        "#   choice and something you would need to consider in your own projects. Can you think of why?\n",
        "   \n",
        "df                         = df.sample(frac=1)\n",
        "train_df, val_df, test_df  = split_train_val_test(df, props=[.8, .1, .1])\n",
        "train_vocab, reverse_vocab = generate_vocab_map(train_df)"
      ],
      "id": "rcmX931OFEId"
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "CAACzA8YFEId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b768c63-2e9f-43a7-bdb2-8f8fc3541027"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8, 0.1, 0.1)"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ],
      "source": [
        "# This line of code will help test your implementation, the expected output is the same distribution used in 'props'\n",
        "#   in the above cell. Try out some different values to ensure it works, but for submission ensure you use \n",
        "#   [.8, .1, .1] \n",
        "\n",
        "(len(train_df) / len(df)), (len(val_df) / len(df)), (len(test_df) / len(df))"
      ],
      "id": "CAACzA8YFEId"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1.4 Building a Dataset Class"
      ],
      "metadata": {
        "id": "5fCFfEHv1hnI"
      },
      "id": "5fCFfEHv1hnI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-qTQQa2FEIe"
      },
      "source": [
        "PyTorch has custom Dataset Classes that have very useful extentions, we want to turn our current pandas DataFrame into a subclass of Dataset so that we can iterate and sample through it for minibatch updates. **In the following cell, fill out the HeadlineDataset class.** Refer to PyTorch documentation on [Dataset Classes](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) \n",
        "for help."
      ],
      "id": "8-qTQQa2FEIe"
    },
    {
      "cell_type": "code",
      "source": [
        "# BEGIN - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "from torch.utils.data import Dataset\n",
        "# END - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "\n",
        "# HeadlineDataset\n",
        "# This class takes a Pandas DataFrame and wraps in a Torch Dataset.\n",
        "# Read more about Torch Datasets here: \n",
        "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "# \n",
        "class HeadlineDataset(Dataset):\n",
        "    \n",
        "    # initialize this class with appropriate instance variables\n",
        "    def __init__(self, vocab, df, max_length=50):\n",
        "        # For this method: We would *strongly* recommend storing the dataframe \n",
        "        #                  itself as an instance variable, and keeping this method\n",
        "        #                  very simple. Leave processing to __getitem__. \n",
        "        #              \n",
        "        #                  Sometimes, however, it does make sense to preprocess in \n",
        "        #                  __init__. If you are curious as to why, read the aside at the \n",
        "        #                  bottom of this cell.\n",
        "        # \n",
        "        \n",
        "        ## YOUR CODE STARTS HERE (~3 lines of code) ##\n",
        "        self.vocab = vocab\n",
        "        self.df = df\n",
        "        self.max_length = max_length\n",
        "        \n",
        "\n",
        "        return \n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "    \n",
        "    # return the length of the dataframe instance variable\n",
        "    def __len__(self):\n",
        "\n",
        "        df_len = None\n",
        "        ## YOUR CODE STARTS HERE (1 line of code) ##\n",
        "        df_len = self.df.shape[0]\n",
        "\n",
        "\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "        return df_len\n",
        "\n",
        "    # __getitem__\n",
        "    # \n",
        "    # Converts a dataframe row (row[\"tokenized\"]) to an encoded torch LongTensor,\n",
        "    # using our vocab map we created using generate_vocab_map. Restricts the encoded \n",
        "    # headline length to max_length.\n",
        "    # \n",
        "    # The purpose of this method is to convert the row - a list of words - into\n",
        "    # a corresponding list of numbers.\n",
        "    #\n",
        "    # i.e. using a map of {\"hi\": 2, \"hello\": 3, \"UNK\": 0}\n",
        "    # this list [\"hi\", \"hello\", \"NOT_IN_DICT\"] will turn into [2, 3, 0]\n",
        "    #\n",
        "    # returns: \n",
        "    # tokenized_word_tensor - torch.LongTensor \n",
        "    #                         A 1D tensor of type Long, that has each\n",
        "    #                         token in the dataframe mapped to a number.\n",
        "    #                         These numbers are retrieved from the vocab_map\n",
        "    #                         we created in generate_vocab_map. \n",
        "    # \n",
        "    #                         **IMPORTANT**: if we filtered out the word \n",
        "    #                         because it's infrequent (and it doesn't exist \n",
        "    #                         in the vocab) we need to replace it w/ the UNK \n",
        "    #                         token\n",
        "    # \n",
        "    # curr_label            - int\n",
        "    #                         Binary 0/1 label retrieved from the DataFrame.\n",
        "    # \n",
        "    def __getitem__(self, index: int):\n",
        "        tokenized_word_tensor = None\n",
        "        curr_label            = None\n",
        "        ## YOUR CODE STARTS HERE (~3-7 lines of code) ##\n",
        "        vocab = self.vocab\n",
        "        curr_data = self.df.iloc[index][\"tokenized\"]\n",
        "        tokenized_word_tensor = torch.LongTensor([vocab[word] if word in vocab else vocab[\"UNK\"] for word in curr_data]) \n",
        "\n",
        "        curr_label = self.df.iloc[index][\"label\"]\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "        return tokenized_word_tensor, curr_label\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "# Completely optional aside on preprocessing in __init__.\n",
        "# \n",
        "# Sometimes the compute bottleneck actually ends up being in __getitem__.\n",
        "# In this case, you'd loop over your dataset in __init__, passing data \n",
        "# to __getitem__ and storing it in another instance variable. Then,\n",
        "# you can simply return the preprocessed data in __getitem__ instead of\n",
        "# doing the preprocessing.\n",
        "# \n",
        "# There is a tradeoff though: can you think of one?\n",
        "# "
      ],
      "metadata": {
        "id": "tqt9q92J1QKK"
      },
      "id": "tqt9q92J1QKK",
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "KuLtIOAZFEIe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "train_dataset = HeadlineDataset(train_vocab, train_df)\n",
        "val_dataset   = HeadlineDataset(train_vocab, val_df)\n",
        "test_dataset  = HeadlineDataset(train_vocab, test_df)\n",
        "\n",
        "# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of PyTorch Random Samplers, they'll\n",
        "#   define how our DataLoaders sample elements from the HeadlineDatasets  \n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "val_sampler   = RandomSampler(val_dataset)\n",
        "test_sampler  = RandomSampler(test_dataset)"
      ],
      "id": "KuLtIOAZFEIe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1.5 Finalizing our DataLoader"
      ],
      "metadata": {
        "id": "n9iBiSKF1yXA"
      },
      "id": "n9iBiSKF1yXA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfXSbxoFFEIe"
      },
      "source": [
        "We can now use PyTorch DataLoaders to batch our data for us. **In the following cell fill out collate_fn.** Refer to PyTorch documentation on [DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for help."
      ],
      "id": "lfXSbxoFFEIe"
    },
    {
      "cell_type": "code",
      "source": [
        "# BEGIN - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# END - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "\n",
        "# collate_fn\n",
        "# This function is passed as a parameter to Torch DataSampler. collate_fn collects\n",
        "# batched rows, in the form of tuples, from a DataLoader and applies some final \n",
        "# pre-processing.\n",
        "#\n",
        "# Objective:\n",
        "# In our case, we need to take the batched input array of 1D tokenized_word_tensors, \n",
        "# and create a 2D tensor that's padded to be the max length from all our tokenized_word_tensors \n",
        "# in a batch. We're moving from a Python array of tuples, to a padded 2D tensor. \n",
        "#\n",
        "# *HINT*: you're allowed to use torch.nn.utils.rnn.pad_sequence (ALREADY IMPORTED)\n",
        "# \n",
        "# Finally, you can read more about collate_fn here: https://pytorch.org/docs/stable/data.html\n",
        "#\n",
        "# args: \n",
        "# batch - PythonArray[tuple(tokenized_word_tensor: 1D Torch.LongTensor, curr_label: int)]\n",
        "#         len(batch) == BATCH_SIZE\n",
        "# \n",
        "# returns:\n",
        "# padded_tokens - 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "# y_labels      - 1D FloatTensor of shape (BATCH_SIZE)\n",
        "# \n",
        "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
        "    padded_tokens, y_labels = None, None\n",
        "    ## YOUR CODE STARTS HERE (~4-8 lines of code) ##\n",
        "    y_labels = torch.FloatTensor([data[1] for data in batch])\n",
        "    \n",
        "    pre_tokens = [data[0] for data in batch]\n",
        "    padded_tokens = pad_sequence(pre_tokens, True, padding_value)    \n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    return padded_tokens, y_labels"
      ],
      "metadata": {
        "id": "Zp1aQAvn1_mz"
      },
      "id": "Zp1aQAvn1_mz",
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "OayoJRTeFEIf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
        "test_iterator  = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "id": "OayoJRTeFEIf"
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "pidbg12AFEIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4f43b6-4ad1-4e85-fada-6d3ab822f5ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 174, 2903, 9461,    1,  321,  563,   36, 2202,    1, 1845, 2854,   12,\n",
            "          223,    1,   33, 7473,    0,    0,    0,    0,    0],\n",
            "        [  11, 1086,  829, 6476,  411,    1,   36,   37,   51,  158, 3298,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [   1, 2698, 8440,   33,  321,   33,  274,   52,    1,   41,  617, 4959,\n",
            "           12, 1017,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [  25,  883,  289,  552,  301,   61,  181,  593,  289, 2774, 2775,   36,\n",
            "         4604, 5446,   36,  113,    1,    1, 1095,   52,    3],\n",
            "        [3472, 4372, 1332,    1, 2430,   52,   57, 8872,    1,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [4223,  120,   15,  431,    1,  217, 5503,   24,    1,  608,   52,    1,\n",
            "          926,    0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [3019,   61, 3210, 2774,    1, 6017, 6636, 4447,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [3123, 4184, 3475,  217, 5633, 8439,   52, 8594, 5089,   21,  605,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [3283,  411,   61, 2461,  158,  476, 3157,  940, 3310, 2128, 6877,  411,\n",
            "           33,  343,  327,   21, 7368,  301,    0,    0,    0],\n",
            "        [  68,  147, 2389,   33, 7046,  952, 8886,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [1642, 5893,   61, 5898,  977, 8704,  133,  263,    1,    1,   56,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [   1, 2710,    1,  165,    1,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [  11,   21, 1022, 6182,  619,  148,  149, 3427,  128, 3880,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [5754, 4651,  321,    1,    1,   21, 1652,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [7450, 6727,  770, 5105, 3296,   36,   81,  784,    1,    1, 6871,  784,\n",
            "          326, 4128,   30,    0,    0,    0,    0,    0,    0],\n",
            "        [1281,   36, 6892, 3256,    1, 1753, 1011,   24, 2486, 4671,   30,   31,\n",
            "         2122,  977,    1, 5918,    1,    0,    0,    0,    0]]) tensor([1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n",
            "x: torch.Size([16, 21])\n",
            "y: torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "# Use this to test your collate_fn implementation.\n",
        "# You can look at the shapes of x and y or put print statements in collate_fn while running this snippet\n",
        "\n",
        "for x, y in test_iterator:\n",
        "     print(x, y)\n",
        "     print(f'x: {x.shape}')\n",
        "     print(f'y: {y.shape}')\n",
        "     break\n",
        "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "id": "pidbg12AFEIf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWLK7T1uFEIg"
      },
      "source": [
        "### Part 2: Modeling [10 pts]\n",
        "Let's move to modeling, now that we have dataset iterators that batch our data for us. In the following code block, you'll build a feed-forward neural network implementing a neural bag-of-words baseline, NBOW-RAND, described in section 2.1 of [this paper](https://www.aclweb.org/anthology/P15-1162.pdf). You'll find [this](https://pytorch.org/docs/stable/nn.html) page useful for understanding the different layers and [this](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) page useful for how to put them into action.\n",
        "\n",
        "The core idea behind this baseline is that after we embed each word for a document, we average the embeddings to produce a single vector that hopefully captures some general information spread across the sequence of embeddings. This means we first turn each document of length *n* into a matrix of *nxd*, where *d* is the dimension of the embedding. Then we average this matrix to produce a vector of length *d*, summarizing the contents of the document and proceed with the rest of the network. \n",
        "\n",
        "While you're working through this implementation, keep in mind how the dimensions change and what each axes represents, as documents will be passed in as minibatches requiring careful selection of which axes you apply certain operations too. You're more than welcome to experiment with the architecture of this network as well outside of the basic setup we describe below, such as adding in other layers, to see how this changes your results."
      ],
      "id": "BWLK7T1uFEIg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2.1 Define the NBOW model class"
      ],
      "metadata": {
        "id": "pZDPs0Sf-H3V"
      },
      "id": "pZDPs0Sf-H3V"
    },
    {
      "cell_type": "code",
      "source": [
        "# BEGIN - DO NOT CHANGE THESE IMPORTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "import torch.nn as nn\n",
        "# END - DO NOT CHANGE THESE IMPORTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "\n",
        "class NBOW(nn.Module):\n",
        "    # Instantiate layers for your model-\n",
        "    # \n",
        "    # Your model architecture will be a feed-forward neural network.\n",
        "    #\n",
        "    # You'll need 3 nn.Modules at minimum\n",
        "    # 1. An embeddings layer (see nn.Embedding)\n",
        "    # 2. A linear layer (see nn.Linear)\n",
        "    # 3. A sigmoid output (see nn.Sigmoid)\n",
        "    #\n",
        "    # HINT: In the forward step, the BATCH_SIZE is the first dimension.\n",
        "    # \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        ## YOUR CODE STARTS HERE (~4 lines of code) ##\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim)\n",
        "        self.W = nn.Linear(embedding_dim, 1)\n",
        "        self.activation = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "        \n",
        "    # Complete the forward pass of the model.\n",
        "    #\n",
        "    # Use the output of the embedding layer to create\n",
        "    # the average vector, which will be input into the \n",
        "    # linear layer.\n",
        "    # \n",
        "    # args:\n",
        "    # x - 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "    #     This is the same output that comes out of the collate_fn function you completed\n",
        "    def forward(self, x):\n",
        "        ## YOUR CODE STARTS HERE (~4-5 lines of code) ##\n",
        "        embedded = self.embedding(x)\n",
        "        output = self.W(embedded)\n",
        "        output = self.activation(output)\n",
        "        \n",
        "        \n",
        "\n",
        "        return output\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "    "
      ],
      "metadata": {
        "id": "jzGx2q0jLqyU"
      },
      "execution_count": 189,
      "outputs": [],
      "id": "jzGx2q0jLqyU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2.2 Initialize the NBOW classification model\n",
        "\n",
        "Since the NBOW model is rather basic, assuming you haven't added any additional layers, there's really only one hyperparameter for the model architecture: the size of the embedding dimension. \n",
        "\n",
        "The vocab_size parameter here is based on the number of unique words kept in the vocab after removing those occurring too infrequently, so this is determined by our dataset and is in turn not a true hyperparameter (though the cutoff we used previously might be). The embedding_dim parameter dictates what size vector each word can be embedded as. \n",
        "\n",
        "If you added additional linear layers to the NBOW model then the input/output dimensions of each would be considered a hyperparameter you might want to experiment with. While the sizes are constrained based on previous & following layers (the number of dimensions need to match for the matrix multiplication), whatever sequence you used could still be tweaked in various ways. \n",
        "\n",
        "A special note concerning the model initialization: We're specifically sending the model to the device set in Part 1, to speed up training if the GPU is available. **Be aware**, you'll have to ensure other tensors are on the same device inside your training and validation loops. "
      ],
      "metadata": {
        "id": "xltosIzM-SP2"
      },
      "id": "xltosIzM-SP2"
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "_HQWUu-ZFEIg"
      },
      "outputs": [],
      "source": [
        "model = NBOW(vocab_size    = len(train_vocab.keys()),\n",
        "             embedding_dim = 300).to(device)"
      ],
      "id": "_HQWUu-ZFEIg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2.3 Instantiate the loss function and optimizer"
      ],
      "metadata": {
        "id": "C4CZnj1f-da-"
      },
      "id": "C4CZnj1f-da-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aXi8nA0FEIh"
      },
      "source": [
        "In the following cell, **select and instantiate an appropriate loss function and optimizer.** \n",
        "\n",
        "Hint: we already use sigmoid in our model. What loss functions are availible for binary classification? Feel free to look at [PyTorch docs](https://pytorch.org/docs/stable/nn.html#loss-functions) for help!"
      ],
      "id": "9aXi8nA0FEIh"
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "w98UvlXxFEIh"
      },
      "outputs": [],
      "source": [
        "#while Adam is already imported, you can try other optimizers as well\n",
        "from torch.optim import Adam\n",
        "\n",
        "criterion, optimizer = None, None\n",
        "### YOUR CODE GOES HERE ###\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.0002)\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE ###"
      ],
      "id": "w98UvlXxFEIh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have a NBOW model to classify headlines as being real or fake and a loss function/optimizer to train the model using the training dataset."
      ],
      "metadata": {
        "id": "hUXBtqPEjiRe"
      },
      "id": "hUXBtqPEjiRe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVLeTa8wFEIh"
      },
      "source": [
        "### Part 3: Training and Evaluation [10 Points]\n",
        "The final part of this HW involves training the model, and evaluating it at each epoch. **Fill out the train and test loops below. Treat real headlines as True, and Onion headlines as False.**  Feel free to look at [PyTorch docs](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html) for help!"
      ],
      "id": "bVLeTa8wFEIh"
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "vganx5fCFEIh"
      },
      "outputs": [],
      "source": [
        "# returns the total loss calculated from criterion\n",
        "def train_loop(model, criterion, optim, iterator):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in tqdm(iterator):\n",
        "        ### YOUR CODE STARTS HERE (~6 lines of code) ###\n",
        "    \n",
        "        # move data from cpu to gpu\n",
        "        gold_labels = y.unsqueeze(1).to(device)\n",
        "        headlines = x.to(device)\n",
        "        # clear gradients\n",
        "        optim.zero_grad()\n",
        "        # predict headlines\n",
        "        predicted_outputs = model(headlines)\n",
        "        # compute loss\n",
        "        loss = criterion(predicted_outputs, gold_labels)\n",
        "        total_loss += loss\n",
        "        # backpropagation\n",
        "        loss.backward()\n",
        "        # weight updates\n",
        "        optim.step()\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "    return total_loss\n",
        "\n",
        "# returns:\n",
        "# - true: a Python boolean array of all the ground truth values \n",
        "#         taken from the dataset iterator\n",
        "# - pred: a Python boolean array of all model predictions. \n",
        "def val_loop(model, iterator):\n",
        "    true, pred = [], []\n",
        "    ### YOUR CODE STARTS HERE (~8 lines of code) ###\n",
        "    for sentences, gold_labels in iterator:\n",
        "        # model prediction: this converting to True False values are actually a redundant comoutation (done it for context clarity)\n",
        "        predicted_outputs = [True if output >= 0.5 else False for output in model(sentences.to(device))]\n",
        "        \n",
        "        # append records: predicted_output (True, False), gold_label (1, 0)\n",
        "        for predicted_output, gold_label in zip(predicted_outputs, gold_labels):\n",
        "            if predicted_output:\n",
        "                pred.append(True)\n",
        "            else:\n",
        "                pred.append(False)\n",
        "            \n",
        "            if int(gold_label):\n",
        "                true.append(True)\n",
        "            else:\n",
        "                true.append(False)\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "    return true, pred"
      ],
      "id": "vganx5fCFEIh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 3.1 Define the evaluation metrics"
      ],
      "metadata": {
        "id": "JNXJevTu-tDZ"
      },
      "id": "JNXJevTu-tDZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IsZQs3rFEIi"
      },
      "source": [
        "We also need evaluation metrics that tell us how well our model is doing on the validation set at each epoch and later how well the model does on the held-out test set. **Complete the functions in the following cell.** You'll find subsection 4.4.1 of Eisenstein useful for this task."
      ],
      "id": "7IsZQs3rFEIi"
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT IMPORT ANYTHING IN THIS CELL. You shouldn't need any external libraries.\n",
        "\n",
        "# accuracy\n",
        "#\n",
        "# What percent of classifications are correct?\n",
        "# \n",
        "# true: ground truth, Python list of booleans.\n",
        "# pred: model predictions, Python list of booleans.\n",
        "# return: percent accuracy bounded between [0, 1]\n",
        "#\n",
        "def accuracy(true, pred):\n",
        "    acc = None\n",
        "    ## YOUR CODE STARTS HERE (~2-5 lines of code) ##\n",
        "    number_of_correct_predictions = sum(1 for prediction, label in zip(pred, true)\n",
        "                                    if prediction == label)\n",
        "    number_of_total_predictions = len(pred)\n",
        "\n",
        "    acc = (number_of_correct_predictions)/(number_of_total_predictions)\n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    return acc\n",
        "\n",
        "# binary_f1 \n",
        "#\n",
        "# A method to calculate F-1 scores for a binary classification task.\n",
        "# \n",
        "# args -\n",
        "# true: ground truth, Python list of booleans.\n",
        "# pred: model predictions, Python list of booleans.\n",
        "# selected_class: Boolean - the selected class the F-1 \n",
        "#                 is being calculated for.\n",
        "# \n",
        "# return: F-1 score between [0, 1]\n",
        "#\n",
        "def binary_f1(true, pred, selected_class=True):\n",
        "    f1 = None\n",
        "    ## YOUR CODE STARTS HERE (~10-15 lines of code) ##\n",
        "    # recall = (# True Positive)/(# True Positive + # of False Negative)\n",
        "    # precision = (# True Positive)/(# True Positive + # of False Positive)\n",
        "    # f1_measure = (2*recall*precision)/(recall + precision)\n",
        "    number_of_true_pos = 0\n",
        "    number_of_false_neg = 0\n",
        "    number_of_false_pos = 0\n",
        "\n",
        "    for predicted, label in zip(pred, true):\n",
        "        if selected_class == label == predicted:\n",
        "            # true positive case\n",
        "            number_of_true_pos += 1\n",
        "\n",
        "        if (selected_class == label) and (label != predicted):\n",
        "            # false negative case\n",
        "            number_of_false_neg += 1\n",
        "\n",
        "        if (selected_class == predicted) and (label != predicted):\n",
        "            # false positive case\n",
        "            number_of_false_pos += 1\n",
        "    #print(f'##########{number_of_true_pos}###########')\n",
        "\n",
        "    # recall\n",
        "    recall = number_of_true_pos/(number_of_true_pos + number_of_false_neg)\n",
        "    # precision\n",
        "    precision = number_of_true_pos/(number_of_true_pos + number_of_false_pos)\n",
        "    # f1 measure\n",
        "    f1 = (2*recall*precision)/(recall + precision)\n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    return f1\n",
        "\n",
        "# binary_macro_f1\n",
        "# \n",
        "# Averaged F-1 for all selected (true/false) classes.\n",
        "#\n",
        "# args -\n",
        "# true: ground truth, Python list of booleans.\n",
        "# pred: model predictions, Python list of booleans.\n",
        "#\n",
        "#\n",
        "def binary_macro_f1(true, pred):\n",
        "    averaged_macro_f1 = None\n",
        "    ## YOUR CODE STARTS HERE (1 line of code) ##\n",
        "    averaged_macro_f1 = (binary_f1(true, pred, True) + binary_f1(true, pred, False))/2\n",
        "\n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    return averaged_macro_f1"
      ],
      "metadata": {
        "id": "gMQDg9Vy-wY0"
      },
      "id": "gMQDg9Vy-wY0",
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "Yw79JFieFEIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03cdfa77-d4bd-495d-de77-f84d711aed4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Macro F1: 0.34252355464685647\n",
            "Accuracy: 0.3975\n"
          ]
        }
      ],
      "source": [
        "# To test your eval implementation, let's see how well the untrained model does on our dev dataset.\n",
        "# It should do pretty poorly, but this can be random because of the initialization of the parameters of the model.\n",
        "true, pred = val_loop(model, val_iterator)\n",
        "print(f'Binary Macro F1: {binary_macro_f1(true, pred)}')\n",
        "print(f'Accuracy: {accuracy(true, pred)}')"
      ],
      "id": "Yw79JFieFEIi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have our datasets defined and split, our model and training tools/loops, and evaluation metrics so we can finally move on to train our model and see how it does!"
      ],
      "metadata": {
        "id": "BerBx-T3kZtC"
      },
      "id": "BerBx-T3kZtC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2to0kWVFEIi"
      },
      "source": [
        "### Part 4: Actually training the model [1 point]\n",
        "Watch your model train :D You should be able to achieve a validation F-1 score of at least .8 if everything went correctly. **Feel free to adjust the number of epochs to prevent overfitting or underfitting and to play with your model hyperparameters/optimizer & loss function.**"
      ],
      "id": "Q2to0kWVFEIi"
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "N-iuqkKCFEIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cadb210-cb2a-485f-f55e-bff75acfe2be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 128.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n",
            "TRAIN LOSS: 754.0433959960938\n",
            "VAL F-1: 0.6060465879754674\n",
            "VAL ACC: 0.6929166666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 127.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "TRAIN LOSS: 680.7566528320312\n",
            "VAL F-1: 0.6650087632040169\n",
            "VAL ACC: 0.72375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 127.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 2\n",
            "TRAIN LOSS: 598.1908569335938\n",
            "VAL F-1: 0.7496567621014978\n",
            "VAL ACC: 0.7791666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 128.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 3\n",
            "TRAIN LOSS: 528.8499145507812\n",
            "VAL F-1: 0.7570565079945266\n",
            "VAL ACC: 0.7858333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 128.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 4\n",
            "TRAIN LOSS: 475.19146728515625\n",
            "VAL F-1: 0.7995138210159637\n",
            "VAL ACC: 0.8166666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 127.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 5\n",
            "TRAIN LOSS: 433.7203063964844\n",
            "VAL F-1: 0.8065049652208032\n",
            "VAL ACC: 0.825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 129.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 6\n",
            "TRAIN LOSS: 403.6098937988281\n",
            "VAL F-1: 0.8321564086856814\n",
            "VAL ACC: 0.8441666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 128.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 7\n",
            "TRAIN LOSS: 374.8362731933594\n",
            "VAL F-1: 0.8418794606332713\n",
            "VAL ACC: 0.8525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 129.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 8\n",
            "TRAIN LOSS: 353.0101318359375\n",
            "VAL F-1: 0.8497835624065907\n",
            "VAL ACC: 0.8595833333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 129.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 9\n",
            "TRAIN LOSS: 333.6537780761719\n",
            "VAL F-1: 0.8572814885143081\n",
            "VAL ACC: 0.8654166666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 129.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 10\n",
            "TRAIN LOSS: 316.3703308105469\n",
            "VAL F-1: 0.8566641110076736\n",
            "VAL ACC: 0.8666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 130.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 11\n",
            "TRAIN LOSS: 300.5408630371094\n",
            "VAL F-1: 0.8600503031599604\n",
            "VAL ACC: 0.8679166666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 129.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 12\n",
            "TRAIN LOSS: 286.3997497558594\n",
            "VAL F-1: 0.8547991938004469\n",
            "VAL ACC: 0.8645833333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 129.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 13\n",
            "TRAIN LOSS: 277.2650146484375\n",
            "VAL F-1: 0.8574960398396717\n",
            "VAL ACC: 0.8679166666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 129.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 14\n",
            "TRAIN LOSS: 264.3644714355469\n",
            "VAL F-1: 0.8668327938623632\n",
            "VAL ACC: 0.8745833333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 129.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 15\n",
            "TRAIN LOSS: 252.5731658935547\n",
            "VAL F-1: 0.8632800648863015\n",
            "VAL ACC: 0.8720833333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 130.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 16\n",
            "TRAIN LOSS: 243.3887176513672\n",
            "VAL F-1: 0.8686584347247346\n",
            "VAL ACC: 0.87625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 129.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 17\n",
            "TRAIN LOSS: 235.56724548339844\n",
            "VAL F-1: 0.8654043358558988\n",
            "VAL ACC: 0.8729166666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 129.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 18\n",
            "TRAIN LOSS: 226.7405548095703\n",
            "VAL F-1: 0.8612852790479649\n",
            "VAL ACC: 0.8695833333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:09<00:00, 129.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 19\n",
            "TRAIN LOSS: 220.14549255371094\n",
            "VAL F-1: 0.8633524206142633\n",
            "VAL ACC: 0.8716666666666667\n"
          ]
        }
      ],
      "source": [
        "TOTAL_EPOCHS = 20\n",
        "for epoch in range(TOTAL_EPOCHS):\n",
        "    train_loss = train_loop(model, criterion, optimizer, train_iterator)\n",
        "    true, pred = val_loop(model, val_iterator)\n",
        "    print(f\"EPOCH: {epoch}\")\n",
        "    print(f\"TRAIN LOSS: {train_loss}\")\n",
        "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
        "    print(f\"VAL ACC: {accuracy(true, pred)}\")"
      ],
      "id": "N-iuqkKCFEIj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l91F4ooFEIj"
      },
      "source": [
        "We can also look at the models performance on the held-out test set, using the same val_loop we wrote earlier."
      ],
      "id": "_l91F4ooFEIj"
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "vs8Fy_ncFEIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d63bf4-2a58-4647-b8d5-075a222915a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TEST F-1: 0.8537071149257807\n",
            "TEST ACC: 0.8654166666666666\n"
          ]
        }
      ],
      "source": [
        "true, pred = val_loop(model, test_iterator)\n",
        "print()\n",
        "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
        "print(f\"TEST ACC: {accuracy(true, pred)}\")"
      ],
      "id": "vs8Fy_ncFEIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMPWmorEFEIp"
      },
      "source": [
        "### Part 5: Analysis [5 points]\n",
        "Answer the following questions:\n",
        "\n"
      ],
      "id": "rMPWmorEFEIp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. What happens to the vocab size as you change the cutoff in the cell below? Can you explain this in the context of [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)?\n",
        "\n",
        "Answer: As can be seen in the bottom graph plotted, as we increase the cutoff value, the size of the vocab decreases drastically. From the graph we can gain an intuition that the cutoff value and the vocab size are in inverse relation. Meaning that there are way more number of words having low frequencies than that of having high frequencies."
      ],
      "metadata": {
        "id": "fnjKlKt352hQ"
      },
      "id": "fnjKlKt352hQ"
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "pI0fM4oMFEIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b15a70-0bee-41e1-aa83-65e7f40521ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13296"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ],
      "source": [
        "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 1)\n",
        "len(tmp_vocab)"
      ],
      "id": "pI0fM4oMFEIp"
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# compute (cutoff value, vocab size) pairs\n",
        "cutoff_vals = [i for i in range(1, 500, 5)]\n",
        "vocab_sizes = [len(generate_vocab_map(train_df, cutoff = cutoff_val)[0]) for cutoff_val in cutoff_vals]\n",
        "\n",
        "# plot data\n",
        "plt.title(\"correlation between vocab size and cutoff value\")\n",
        "plt.xlabel(\"cutoff value\")\n",
        "plt.ylabel(\"vocab size\")\n",
        "plt.plot(cutoff_vals, vocab_sizes)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Oyg8RUNlqEAZ",
        "outputId": "64f5ee36-d6a8-46b4-bc7a-3cdcaa9588ae"
      },
      "id": "Oyg8RUNlqEAZ",
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0x54B1lFEIp"
      },
      "source": [
        "#### 2. Can you describe what cases the model is getting wrong in the witheld test-set? \n",
        "\n",
        "Answer: In the case where the words in the given sentence is not in the vocab dictionary, meaning that for cases where the model cannot identify the words (\"UNK\"), it seems like the model has less countable predictions over the given data. Also, one problem with the feedforward nn is that it does not deal with the given sentence as sequeuntial data. Meaning that the sequence (ordering) of the words in a given sentence do influence the contextual meaning of the sentence and thus the model could show weakness upon understandig the correlation between the previous words to posterior words in the sentences. "
      ],
      "id": "d0x54B1lFEIp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do this, you'll need to create a new val_train_loop (``val_train_loop_incorrect``) so it returns incorrect sequences **and** you'll need to decode these sequences back into words. \n",
        "Thankfully, you've already created a map that can convert encoded sequences back to regular English: you will find the ``reverse_vocab`` variable useful.\n",
        "\n",
        "```\n",
        "# i.e. using a reversed map of {\"hi\": 2, \"hello\": 3, \"UNK\": 0}\n",
        "# we can turn [2, 3, 0] into this => [\"hi\", \"hello\", \"UNK\"]\n",
        "```"
      ],
      "metadata": {
        "id": "_yiIZov-583w"
      },
      "id": "_yiIZov-583w"
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "TfohtPF8FEIp"
      },
      "outputs": [],
      "source": [
        "# Implement this however you like! It should look very similar to val_loop.\n",
        "# Pass the test_iterator through this function to look at errors in the test set.\n",
        "\n",
        "# using reverse_vocab built earlier in pre-processing step\n",
        "def val_train_loop_incorrect(model, iterator):\n",
        "    decoded_incorrect_predictions = []\n",
        "    for sentences, gold_labels in iterator:\n",
        "        # model predictions: this converting to True False values are actually a redundant comoutation (done it for context clarity)\n",
        "        predicted_outputs = [True if output >= 0.5 else False for output in model(sentences.to(device))]\n",
        "\n",
        "        for sentence, predicted_output, gold_label in zip(sentences, predicted_outputs, gold_labels):\n",
        "            if predicted_output is (not int(gold_label)): # parentheses were necessary for (not gold_label) otherwise produced unexpected outcomes\n",
        "                # append mis-predicted inputs\n",
        "                decoded_incorrect_predictions.append([reverse_vocab[int(word_id)] for word_id in sentence]) # note that word_id is in tensor. type cast to int\n",
        "\n",
        "    return decoded_incorrect_predictions\n",
        "   "
      ],
      "id": "TfohtPF8FEIp"
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_predictions = val_train_loop_incorrect(model, test_iterator)\n",
        "\n",
        "for incorrect_prediction in incorrect_predictions:\n",
        "    print(incorrect_prediction)"
      ],
      "metadata": {
        "id": "6-azPje88iU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff8a8acd-5022-40ff-cc7d-d14274fecbc7"
      },
      "id": "6-azPje88iU0",
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['report', ':', 'would-be', 'suicide', 'UNK', 'pushed', 'off', 'bridge', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['study', ':', 'women', 'fake', 'UNK', 'to', 'increase', 'sexual', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['devotees', 'visit', 'UNK', 'to', 'get', 'UNK', 'marked', 'with', 'syrup', 'cross', 'on', 'national', 'pancake', 'day', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['secretary', 'of', 'state', 'fired', 'after', 'UNK', 'weighing', 'in', 'on', 'international', 'politics', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['in', 'america', \"'s\", 'UNK', ',', 'baseball', 'players', 'pass', 'a', 'lot', 'of', 'time', '.', 'the', 'UNK', ':', '90', '%', 'of', 'the', 'game', 'is', 'spent', 'standing', 'around', '.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['‘', 'join', 'email', 'list', '’', 'box', 'UNK', 'like', 'UNK', ',', 'UNK', 'UNK', 'it', 'is', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['breaking', ':', 'situation', 'UNK', 'in', 'venezuela', ',', 'UNK', ',', 'u.s.', ',', 'japan', ',', 'mexico', ',', 'iraq', ',', 'spain', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['friend', \"'s\", 'UNK', 'for', 'why', 'he', 'ca', \"n't\", 'hang', 'out', 'getting', 'more', 'UNK', 'over', 'time', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['scientists', 'UNK', 'about', 'UNK', 'UNK', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['pizza', 'hut', 'unveils', 'new', 'UNK', 'delivery', 'boy', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['father', 'and', 'son', 'fight', 'over', 'an', 'onion', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['this', 'man', 'built', 'himself', 'a', 'plane', 'so', 'he', 'could', 'save', '7', 'minutes', 'off', 'his', 'office', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'horses', 'meet', 'under', 'cover', 'of', 'darkness', 'for', 'kentucky', 'street', 'derby', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['betsy', 'devos', 'confirmed', 'as', 'education', 'secretary', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['christ', 'sues', 'catholic', 'church', 'for', 'unlicensed', 'use', 'of', 'his', 'image', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', '’', 's', 'martin', 'shkreli', 'regrets', '5,000', '%', 'price', 'UNK', 'it', 'wasn', '’', 't', 'high', 'enough', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['sears', 'UNK', 'to', 'earn', 'thousands', 'in', 'UNK', 'while', 'UNK', 'workers', 'get', 'no', 'UNK', '', '', '', '', '', '', '', '']\n",
            "['man', 'on', 'verge', 'of', 'UNK', 'instead', 'turns', 'to', 'god', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['food', 'purchased', 'as', 'souvenir', 'tragically', 'revealed', 'to', 'be', 'available', 'back', 'home', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['scientists', 'have', 'discovered', 'what', 'causes', 'UNK', 'bitch', 'face', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['nasa', 'says', 'presence', 'of', 'diving', 'board', 'on', 'mars', 'confirms', 'planet', 'may', 'have', 'once', 'contained', 'water', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'business', 'discriminates', 'against', 'customers', 'who', 'just', 'have', 'UNK', 'goats', 'to', 'UNK', 'with', '', '', '', '', '', '', '', '', '', '']\n",
            "['mood', 'in', 'car', 'takes', 'grim', 'turn', 'after', 'dad', 'UNK', 'exit', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['hillary', 'reaches', 'base', 'with', 'UNK', 'UNK', 'page', 'ad', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'UNK', 'recalls', 'having', 'to', 'torture', 'more', 'prisoners', 'than', 'male', 'colleagues', 'to', 'prove', 'herself', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[\"'he\", 'made', 'the', 'ultimate', 'sacrifice', ',', \"'\", 'trump', 'tells', 'military', 'widow', 'about', 'UNK', 'putting', 'up', 'with', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['jared', 'kushner', 'unsure', 'whether', 'he', \"'d\", 'alert', 'fbi', 'if', 'russians', 'request', 'another', 'meeting', '', '', '', '', '', '', '']\n",
            "['humiliated', 'man', 'discovers', 'UNK', 'on', 'his', 'UNK', 'pockets', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['peaceful', 'protest', 'interrupted', 'by', 'swarm', 'of', 'aggressive', 'UNK', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['u.s.', 'UNK', '12', 'russian', 'officials', 'who', 'will', 'be', 'indicted', 'for', '2018', ',', '2020', 'election', 'hacking', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['papa', 'john', '’', 's', 'removes', 'n-word', 'from', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['nra', 'visits', 'colorado', 'police', 'evidence', 'room', 'to', 'check', 'up', 'on', 'rifle', 'used', 'in', 'planned', 'parenthood', 'shooting', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'UNK', 'cover', 'band', 'drops', 'out', 'of', 'inauguration', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'former', 'spelling', 'bee', 'champion', 'sitting', 'in', 'front', 'of', 'tv', 'sadly', 'UNK', 'along', 'with', 'UNK', 'contestants', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['antonio', 'UNK', 'welcomes', 'third', 'child', 'since', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['son', 'needs', 'costume', ',', '30', 'UNK', 'wrapped', 'treats', 'tomorrow', 'morning', 'for', 'some', 'school', 'celebration', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['online', 'university', 'allows', 'students', 'to', 'UNK', 'UNK', 'debt', 'at', 'own', 'pace', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['frustrated', 'writer', 'tosses', 'another', 'UNK', 'laptop', 'in', 'trash', 'can', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['highlights', 'from', '‘', 'go', 'set', 'a', 'UNK', '’', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['gop', 'leaders', 'demand', 'congressman', 'duncan', 'hunter', '’', 's', 'resignation', 'after', 'discovering', 'he', 'poor', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['airbnb', 'host', 'decides', 'UNK', 'note', 'necessary', 'to', 'protect', 'cocktail', 'sauce', 'in', 'fridge', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['new', 'obesity', 'drug', 'delicious', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['no', 'one', 'in', 'gang', 'has', 'heart', 'to', 'tell', 'police', 'UNK', 'his', 'cover', \"'s\", 'blown', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['obamas', 'sign', 'UNK', 'deal', 'with', 'spotify', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['supreme', 'court', ':', 'UNK', 'can', 'be', 'UNK', 'for', 'racism', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['old', 'state', 'department', 'library', 'now', 'UNK', 'of', 'nothing', 'but', 'donald', 'trump', 'books', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'UNK', 'itself', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'bans', 'all', 'redskins', 'items', ',', 'allows', 'UNK', 'swastika', 'items', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['empty', '‘', 'about', 'us', '’', 'page', 'leaves', 'chinese', 'buffet', '’', 's', 'UNK', 'UNK', 'in', 'mystery', '', '', '', '', '', '', '', '', '', '']\n",
            "['mike', 'pence', 'asks', 'waiter', 'to', 'remove', 'mrs.', 'UNK', 'from', 'table', 'until', 'wife', 'arrives', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['americans', 'favor', 'legalizing', 'pot', 'and', 'UNK', 'congress', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['donald', 'trump', 'rejected', 'secretary', 'of', 'state', 'candidate', 'john', 'bolton', 'over', 'his', 'UNK', ',', 'adviser', 'reveals', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'restaurant', 'proudly', 'serves', 'UNK', 'tortured', 'animals', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['anthony', 'UNK', 'did', ',', 'in', 'fact', ',', 'read', 'that', 'fan', 'fiction', 'about', 'him', 'doing', 'no', 'UNK', 'in', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'man', 'murders', 'controlling', 'wife', 'in', 'lifetime', 'for', 'men', 'original', 'movie', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['4', 'UNK', 'realized', 'hours', 'into', 'brain', 'surgery', 'that', 'they', 'were', 'UNK', 'on', 'the', 'wrong', 'patient', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['florida', 'resort', 'allows', 'guests', 'to', 'swim', 'with', 'miami', 'dolphins', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['chinese', 'citizens', 'kind', 'of', 'grateful', 'to', 'not', 'have', 'access', 'to', 'all', 'of', 'internet', '', '', '', '', '', '', '', '', '']\n",
            "['are', 'your', 'parents', 'letting', 'you', 'get', 'a', 'tattoo', 'because', 'they', 'don', '’', 't', 'care', 'in', 'a', 'cool', 'way', 'or', 'in', 'a', 'sad', 'way', '?']\n",
            "['abc', 'admits', 'that', 'they', 'never', 'read', 'UNK', 'emails', 'that', 'they', 'UNK', 'obama', 'with', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['apple', '‘', 'hipsters', '’', 'UNK', 'that', 'the', 'company', 'was', 'better', 'before', 'it', 'became', 'cool', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['scandal', '!', 'player', 'caught', 'hiding', 'blank', 'UNK', 'at', 'UNK', 'national', 'championships', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['bbc', 'UNK', 'lose', 'new', 'UNK', 'coin', 'down', 'the', 'back', 'of', 'the', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['area', 'woman', 'emotionally', 'UNK', 'in', 'jennifer', 'UNK', \"'s\", 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['infowars', 'moves', 'to', 'ban', 'alex', 'jones', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'paul', ':', \"'i\", 'didn', '’', 't', 'realize', 'people', 'who', 'commit', 'suicide', 'kill', 'themselves', \"'\", '', '', '', '', '', '', '', '', '', '']\n",
            "['secret', 'facebook', 'group', 'posts', 'nude', 'photos', 'of', 'female', 'marines', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'has', 'wikipedia', 'page', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['sick', ',', 'elderly', 'man', 'screaming', 'about', 'foreigners', 'stealing', 'from', 'him', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['coca-cola', 'is', 'bringing', 'back', 'new', 'coke', 'in', 'honor', 'of', 'UNK', 'things', \"'\", '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['trump', ',', 'putin', 'hold', 'first', 'joint', 'press', 'crackdown', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['south', 'carolina', 'refuses', 'to', 'remove', 'confederate', 'flag', 'from', 'capitol', 'trailer', '', '', '', '', '', '', '']\n",
            "['UNK', 'is', 'UNK', 'its', 'UNK', '/', '10', 'for', '‘', 'UNK', '5', '’', 'after', 'realizing', 'we', 'haven', '’', 't', 'gotten', 'a', 'single', 'dollar', 'from', 'microsoft', '', '', '', '', '', '', '', '']\n",
            "['scott', 'walker', '’', 's', 'office', 'unable', 'to', 'provide', 'written', 'proof', 'of', 'his', 'communications', 'with', 'god', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['teen', 'on', 'UNK', 'of', 'UNK', 'incredible', 'UNK', 'of', 'UNK', 'instead', 'asks', 'boyfriend', 'to', 'use', 'condom', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['islamic', 'state', 'to', 'john', 'kerry', ':', 'you', '’', 're', 'an', '‘', 'old', 'UNK', 'UNK', '’', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['melania', 'trump', 'would', 'have', 'been', 'UNK', 'for', 'deportation', 'under', 'new', 'immigration', 'rules', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['so', 'long', ',', 'UNK', ':', 'man', 'UNK', 'out', 'part', 'of', 'toy', 'UNK', 'after', '44', 'years', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['federal', 'government', 'adds', '600,000', 'acres', 'to', 'national', 'forbidden', 'zone', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['u.s.', 'military', 'clears', 'UNK', 'of', 'charges', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['perfect', 'day', 'in', ':', 'get', 'UNK', 'UNK', 'into', 'killing', 'a', 'k-pop', 'star', ',', 'and', '4', 'other', 'plans', 'for', 'a', 'perfect', 'day', 'in', 'seoul']\n",
            "['pope', 'francis', 'beats', 'confession', 'out', 'of', 'UNK', 'catholic', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['biden', 'gets', 'grow', 'light', 'delivered', 'to', 'white', 'house', 'under', 'fake', 'name', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['chuck', 'UNK', ':', 'if', 'obama', 'is', 'UNK', ',', 'it', 'could', 'bring', 'about', '‘', 'one', 'thousand', 'years', 'of', 'darkness', '’', '', '', '', '']\n",
            "['facebook', 'promises', 'to', 'stop', 'asking', 'you', 'to', 'wish', 'happy', 'birthday', 'to', 'your', 'friend', 'who', 'died', '', '', '', '', '', '', '', '']\n",
            "['greatest', 'genius', 'in', 'cow', 'history', 'killed', ',', 'eaten', '', '', '', '', '']\n",
            "['self-conscious', 'panda', 'swears', 'it', 'UNK', 'UNK', 'UNK', 'to', 'it', 'as', 'UNK', \"'\", '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['old', 'UNK', 'phones', 'find', 'new', 'life', 'as', 'sex', 'toys', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['sesame', 'street', ':', 'UNK', 'and', 'ernie', 'are', 'not', 'gay', ',', 'they', 'are', 'UNK', 'UNK', 'UNK', \"'\", '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['man', 'with', 'first', 'name', '‘', 'god', '’', 'runs', 'into', 'UNK', 'issues', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['a', 'candle', 'that', 'smells', 'like', 'a', 'new', 'UNK', 'exists', '', '', '', '', '', '', '', '', '', '']\n",
            "['scientists', 'fear', 'female', 'UNK', 'UNK', 'too', 'effective', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['depressed', 'businessman', 'takes', '16', 'power', 'naps', 'a', 'day', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['sweating', ',', 'shaking', 'UNK', 'ceo', 'says', 'he', 'can', 'stop', 'UNK', 'off', 'opioid', 'epidemic', 'UNK', 'he', 'wants', '', '', '']\n",
            "['find', 'out', 'what', 'justin', 'bieber', ',', 'jennifer', 'UNK', ',', 'and', 'UNK', 'love', 'have', 'to', 'say', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['prominent', 'white', 'supremacist', 'UNK', 'on', 'video', 'by', 'his', 'father', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['$', 'UNK', 'UNK', 'jeans', 'little', 'more', 'than', 'UNK', 'of', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'UNK', 'to', 'be', 'allowed', 'to', 'sound', 'their', 'UNK', 'more', 'regularly', 'and', 'for', 'longer', 'after', 'government', 'UNK', '', '', '', '', '', '', '', '', '', '']\n",
            "['obama', 'asked', 'hbo', 'chief', 'for', '‘', 'true', 'detective', ',', '’', '‘', 'game', 'of', 'thrones', '’', 'advance', 'episodes', '', '', '', '', '', '', '', '', '', '']\n",
            "['u.s.', 'forest', 'service', 'kills', 'off', 'UNK', 'bear', 'to', 'get', 'people', 'serious', 'about', 'fire', 'safety', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['japanese', 'man', 'born', 'to', 'wealthy', 'parents', 'is', 'accidentally', 'switched', 'at', 'birth', 'and', 'UNK', 'life', 'of', 'poverty', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['youtube', 'UNK', 'to', 'shut', 'down', 'school', 'shooter', '’', 's', 'account', 'over', 'copyright', 'complaints', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['jeb', 'bush', ':', 'people', 'need', 'to', 'work', 'longer', 'hours', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['rats', 'laugh', 'when', 'UNK', 'UNK', ',', 'top', 'scientists', 'reveal', '', '', '', '', '', '', '', '']\n",
            "['professor', ':', 'if', 'you', 'read', 'to', 'your', 'kids', ',', 'you', '’', 're', '‘', 'UNK', 'UNK', '’', 'others']\n",
            "['queen', 'elizabeth', 'to', 'celebrate', 'her', 'UNK', 'birthday', 'by', 'getting', 'UNK', 'by', 'shaggy', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['video', ':', 'two', 'years', 'ago', ',', 'this', 'man', 'was', '500', 'pounds', '.', 'now', 'he', 'is', 'two', 'men', 'who', 'weigh', '250', 'pounds', '.', '']\n",
            "['dog', 'dumped', 'by', 'UNK', 'with', 'her', 'dead', 'UNK', 'in', 'a', 'bag', 'UNK', 'tears', 'in', 'her', 'eyes', \"'\", '', '', '', '', '', '', '', '', '', '']\n",
            "['man', 'figured', 'drug', 'addiction', 'would', 'take', 'up', 'a', 'lot', 'more', 'free', 'time', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['meat', 'prices', 'UNK', 'after', 'cow', 'smashing', 'machine', 'gets', 'all', 'UNK', 'up', '(', 'video', ')', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['is', 'your', 'interior', 'designer', 'putting', 'your', 'life', 'at', 'risk', '?', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['judge', 'sentences', 'UNK', 'UNK', 'to', '100', 'hours', 'of', 'community', 'theater', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['small', 'UNK', 'airline', 'realizes', 'that', 'marketing', 'would', 'be', 'a', 'good', 'idea', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['president', 'donald', 'UNK', 'trump', 'UNK', 'april', '2017', 'as', 'national', 'sexual', 'assault', 'awareness', 'and', 'prevention', 'month', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'and', 'virtual', 'UNK', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['10', 'beautiful', 'interracial', 'arrests', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['target', '‘', 'dorm', 'room', 'UNK', '’', 'aisle', 'being', 'UNK', 'exclusively', 'by', 'UNK', 'men', 'with', 'studio', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['drunk', 'will', 'show', 'you', ',', 'everybody', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['cyclist', 'celebrates', 'win', 'one', 'lap', 'too', 'early', ',', 'gets', 'passed', 'by', 'others', ',', 'UNK', 'UNK', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'UNK', 'hires', 'jeff', 'UNK', 'to', 'walk', 'around', ',', 'being', 'UNK', 'UNK', 'by', 'things', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['taco', 'bell', 'unveils', 'new', 'taco', 'with', 'UNK', 'made', 'from', 'doritos', 'bags', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['comedian', 'UNK', 'as', 'ukraine', '’', 's', 'new', 'president', 'immediately', 'UNK', 'parliament', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['government', 'delays', 'new', 'pornography', 'regulation', 'as', 'it', 'works', 'out', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['giuliani', 'says', 'kim', 'jong-un', 'UNK', 'like', 'a', 'UNK', 'trying', 'to', 'get', 'a', 'job', 'at', 'the', 'white', 'house', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['an', 'interesting', 'title', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['power', 'rangers', 'star', 'charged', 'with', 'UNK', 'call', 'UNK', '!', '(', 'but', 'seriously', ',', 'someone', 'is', 'dead', ')', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['``', 'new', 'subway', 'promotion', 'to', 'honor', 'UNK', '11', \"''\", '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['lawyers', 'identify', 'dozens', 'more', 'bill', 'cosby', 'victims', 'while', 'UNK', 'potential', 'UNK', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['dad', \"'s\", 'previously', 'UNK', 'friend', 'dies', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['matt', 'damon', 'UNK', 'accepts', '$', '50', 'in', 'ebay', 'auction', 'for', '‘', 'the', 'UNK', '’', 'cast', 'and', 'crew', 'jacket', '', '', '', '']\n",
            "['news', ':', 'a', 'UNK', 'of', 'justice', ':', 'a', 'shocking', 'study', 'has', 'found', 'that', 'as', 'many', 'as', '1', 'in', '10', 'people', 'burned', 'at', 'the', 'stake', 'for', 'witchcraft', 'is', 'falsely', 'accused']\n",
            "['police', 'found', 'golden', 'state', 'killer', 'by', 'UNK', 'owner', 'of', '‘', 'UNK', '’', 'website', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['sexy', 'girls', 'for', 'hot', 'sex', 'here', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['popular', 'children', \"'s\", 'book', 'author', 'reveals', 'the', 'UNK', 'truth', \"'\", 'about', 'creepy', 'conspiracy', 'theories', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['conspiracy', 'theorist', 'has', 'elaborate', 'explanation', 'for', 'why', 'he', \"'s\", 'single', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['proud', 'billionaire', 'helps', 'young', 'son', 'open', 'first', 'UNK', 'bank', 'account', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['fifty', 'years', 'ago', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['super', 'pac', 'to', 'get', 'rid', 'of', 'super', 'UNK', 'raises', '$', 'UNK', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'father', 'of', 'UNK', 'receives', 'shocking', 'news', 'in', 'the', 'delivery', 'room', ':', 'there', 'are', 'no', 'babies', '', '', '', '']\n",
            "['the', 'man', 'with', 'a', 'bionic', 'penis', 'will', 'lose', 'his', 'virginity', 'to', 'a', 'dominatrix', 'who', 'ran', 'for', 'parliament', '', '', '']\n",
            "['[', 'theonion', ']', 'this', 'is', 'not', 'a', 'dating', 'site', '.', 'largest', 'in', 'world', 'online', 'search', 'sex', 'UNK', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['four', 'UNK', 'do', 'a', 'base', 'jump', 'from', 'UNK', 'towers', 'after', 'UNK', 'at', 'UNK', 'de', 'UNK', 'bar', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['news', ':', 'just', 'like', 'us', ':', 'this', 'UNK', 'shot', 'himself', 'in', 'the', 'head', 'after', 'the', 'irs', 'uncovered', 'his', 'tax', 'fraud', 'scheme', '', '', '', '', '']\n",
            "['colorado', 'boy', 'needs', 'help', 'finding', 'the', 'best', 'shirt', 'ever', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['old', 'friends', 'from', 'high', 'school', 'meet', 'up', 'every', 'year', 'to', 'say', 'names', 'of', 'former', 'classmates', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['alcohol', 'UNK', 'blamed', 'for', 'local', 'man', '’', 's', 'impaired', 'judgment', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'ride', 'UNK', 'to', 'UNK', 'win', 'over', 'pirates', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['silicon', 'valley', 'startup', 'seeks', 'to', 'change', 'the', 'way', 'women', 'flee', 'tech', 'industry', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['supreme', 'court', 'rules', 'supreme', 'court', 'rules', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['impossible', 'for', '4', 'people', 'to', 'rape', 'someone', 'together', ':', 'UNK', 'UNK', 'UNK', '', '', '', '', '', '', '', '', '']\n",
            "['cat', 'lives', 'double', 'life', 'with', 'second', 'family', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['trump', 'leads', 'gop', 'presidential', 'field', 'in', 'new', 'national', 'poll', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['jeb', 'bush', 'speech', 'UNK', 'lobbyists', 'was', 'UNK', 'by', 'corporate', 'lobbying', 'group', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['13-year-old', 'drinking', 'prodigy', 'UNK', 'to', 'ohio', 'state', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['area', 'satirical', 'publication', 'the', 'onion', 'sold', 'to', 'UNK', '(', 'seriously', ')', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['russian', 'olympic', 'officials', 'concerned', 'after', 'learning', 'UNK', 'clean', 'urine', 'UNK', 'almost', 'empty', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['huge', 'UNK', 'of', 'UNK', 'coins', 'reveals', 'medieval', 'tax', 'scam', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', '&', '&', 'UNK', 'ceo', 'explains', 'why', 'he', 'hates', 'fat', 'UNK', '', '', '', '', '', '', '', '', '', '']\n",
            "['tea', 'party', 'nation', 'still', 'trying', 'to', 'make', 'romney', 'president', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['pumpkin', 'spice', 'UNK', 'UNK', '4', 'people', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['we', 'would', 'like', 'to', 'make', 'the', 'following', 'UNK', 'to', 'our', '‘', '13', 'hedgehogs', 'who', 'need', 'a', 'vacation', '’', 'list', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'police', 'bring', 'in', 'stephen', 'king', 'to', 'help', 'track', 'demonic', 'car', 'that', 'killed', 'woman', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['this', 'gop', 'house', 'candidate', 'proposed', 'UNK', 'the', 'weekend', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['bank', 'of', 'america', ':', 'there', '’', 's', 'a', '20', '%', 'UNK', '%', 'chance', 'we', '’', 're', 'inside', 'the', 'matrix', 'and', 'reality', 'is', 'just', 'a', 'simulation']\n",
            "['nintendo', 'announces', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['hundreds', 'of', 'people', 'gather', 'to', 'say', '‘', 'wow', '’', 'like', 'UNK', 'wilson', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['when', 'your', 'UNK', 'friends', 'ca', \"n't\", 'spot', 'a', 'joke', 'article', '.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['call', 'me', 'a', 'radical', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['congress', 'returns', 'to', 'work', 'to', 'do', 'the', 'bare', 'minimum', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'woman', 'lies', 'about', 'her', 'age', 'to', 'join', 'facebook', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['new', 'evidence', 'suggests', 'president', 'george', 'washington', 'sent', 'UNK', 'of', 'penis', 'to', 'secretary', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'ordered', 'to', 'pull', 'rabbit', 'disaster', 'plan', 'out', 'of', 'his', 'hat', 'after', 'he', 'was', 'forced', 'to', 'buy', 'special', 'bunny', 'licence', '', '', '', '', '', '', '']\n",
            "['‘', 'now', 'i', 'understand', 'how', 'nazi', 'germany', 'happened', ',', '’', 'says', 'UNK', 'man', 'finally', 'playing', '‘', 'UNK', '3d', '’', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'that', 'UNK', 'in', 'box', 'too', 'fancy', 'for', 'dry', 'food', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['kentucky', 'players', 'UNK', 'over', 'losing', 'UNK', 'season', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['arby', \"'s\", 'now', 'charging', '$', 'UNK', 'to', 'let', 'customers', 'go', 'behind', 'counter', ',', 'grab', 'UNK', 'of', 'roast', 'beef', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['woman', 'can', '’', 't', 'wait', 'to', 'get', 'home', 'and', 'take', 'off', 'uncomfortable', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['news', ':', 'crisis', ':', 'the', 'astronauts', 'on', 'the', 'UNK', 'can', '’', 't', 'get', 'their', 'drug', 'dealer', 'to', 'leave', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'statements', 'made', 'by', 'ted', 'cruz', 'were', 'UNK', '–', 'the', 'only', 'one', 'that', 'was', 'true', 'was', 'about', 'toilets']\n",
            "['louis', 'UNK', '.', 'fan', 'disappointed', 'at', 'lack', 'of', 'UNK', 'power', 'games', 'in', 'new', 'material', '', '', '', '']\n",
            "['fidel', 'castro', 'dies', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['cat', 'congress', 'UNK', 'in', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['ben', 'carson', 'showed', 'his', 'UNK', 'side', 'on', 'thursday', 'by', 'pulling', 'a', 'UNK', ',', 'UNK', 'joke', 'on', 'the', 'media', 'while', 'giving', 'a', 'speech', 'on', 'which', 'he', 'UNK', 'as', '``', 'getting', 'out', 'our', 'UNK', 'and', 'making', 'some', 'UNK', \"'\", \"''\", '.']\n",
            "['cop', 'hired', 'for', 'posting', 'racist', 'rant', 'on', 'social', 'media', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'skeleton', 'UNK', 'senate', 'desk', 'expected', 'to', 'seek', 'UNK', 'term', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['trump', 'UNK', 'pope', 'on', 'climate', 'change', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['mitt', 'romney', 'UNK', 'accepts', 'thing', 'he', 'has', 'paid', 'millions', 'of', 'dollars', 'for', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['starbucks', 'ending', 'alcohol', 'service', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['40', 'year', 'old', 'has', 'UNK', 'hair', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['‘', 'prince', 'george', 'effect', '’', 'leads', 'to', 'UNK', 'UNK', 'sales', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['‘', 'secretary', 'clinton', 'is', 'a', 'different', 'person', 'than', 'donald', 'trump', ',', '’', 'says', 'bernie', 'sanders', 'in', 'UNK', 'endorsement', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['black', 'woman', 'drives', 'to', 'new', 'orleans', 'to', 'make', 'sure', '‘', 'UNK', 'davis', 'lives', 'to', 'see', 'another', 'day', '’', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['president', 'trump', 'orders', 'his', 'steak', 'well', 'done', 'with', 'ketchup', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['manager', 'of', 'UNK', 'taco', 'bell', '/', 'kfc', 'secretly', 'considers', 'it', 'mostly', 'a', 'taco', 'bell', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'brooks', 'starts', 'foundation', 'to', 'save', 'word', 'UNK', \"'\", '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['meat', 'prices', 'UNK', 'after', 'cow', 'smashing', 'machine', 'gets', 'all', 'UNK', 'up', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['driver', 'kind', 'of', 'UNK', 'to', 'see', 'other', 'car', 'he', 'been', 'driving', 'behind', 'for', 'a', 'while', 'take', 'exit', 'off', 'highway', '', '', '', '', '', '', '']\n",
            "['safe', 'sealed', 'for', '40', 'years', 'until', 'museum', 'visitor', 'UNK', 'the', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['parents', 'brawl', 'during', 'youth', 'baseball', 'game', 'after', 'UNK', 'with', '13-year-old', 'umpire', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['report', ':', 'metallica', 'and', 'other', 'UNK', 'caught', 'UNK', 'with', 'live', 'nation', 'to', 'UNK', 'ticket', 'UNK', 'UNK', 'thousands', 'of', 'UNK', 'directly', 'to', 'UNK', 'sites']\n",
            "['michael', 'bay', 'to', 'produce', 'UNK', '‘', 'UNK', 'the', 'explorer', '’', 'movie', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['man', 'that', 'dreamed', 'of', 'opening', 'UNK', 'studio', 'becomes', 'UNK', 'after', 'UNK', 'accident', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['woman', 'lets', 'god', 'drive', 'car', ',', 'god', 'immediately', 'runs', 'down', 'guy', 'on', 'motorcycle', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'goat', 'dies', 'following', 'failed', 'everest', 'climb', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['mom', 'wants', 'to', 'know', 'if', 'you', '’', 'll', 'be', 'free', 'if', 'she', 'visits', '14', 'months', 'from', 'now', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['these', 'brave', 'teens', 'went', 'UNK', 'for', '3', 'whole', 'days', 'and', 'miraculously', 'survived', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['libertarian', 'party', 'chair', 'candidate', 'strips', 'on', 'stage', 'during', 'national', 'convention', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', '/', '11', 'conspiracy', 'theories', 'ridiculous', \"'\", '-', 'al', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['a', 'crisis', 'of', 'faith', ':', 'pope', 'francis', 'has', 'left', 'the', 'catholic', 'church', 'to', 'worship', 'a', 'monster', 'truck', 'he', 'saw', 'UNK', '30', 'school', 'UNK', '', '', '', '', '', '', '', '']\n",
            "['police', 'investigate', 'reports', 'of', 'local', 'gay', 'man', 'being', 'UNK', 'behind', 'boat', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['on', 'UNK', 'of', '0', 'to', '500', ',', 'beijing', '’', 's', 'air', 'quality', 'tops', '‘', 'crazy', 'bad', '’', 'at', 'UNK', '', '']\n",
            "['man', 'somehow', 'getting', 'worse', 'at', 'sex', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['nearly', '100,000', 'new', 'yorkers', 'sign', 'petition', 'to', 'get', 'rid', 'of', 'melania', 'trump', '', '', '', '', '', '']\n",
            "['UNK', 'clothes', 'UNK', 'cameras', 'by', 'making', 'you', 'look', 'like', 'a', 'car', '', '', '', '', '', '', '']\n",
            "['lone', 'person', 'UNK', 'UNK', 'at', 'UNK', 'of', 'obama', 'summit', 'remarks', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['fox', 'news', 'wonders', ':', 'why', 'don', '’', 't', 'our', 'athletes', 'love', 'america', 'more', '?', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['notre', 'dame', 'beekeeper', 'waits', 'to', 'learn', 'fate', 'of', 'his', 'at', 'least', 'UNK', 'bees', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['experts', 'suggest', '9', 'weed', 'UNK', 'to', 'get', 'you', 'through', 'a', 'presidential', 'debate', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['high', 'UNK', ',', 'moral', 'UNK', 'has', 'cost', 'idiot', 'man', 'millions', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['news', ':', 'UNK', 'on', 'spending', '?', 'it', 'costs', '$', '18', 'million', 'a', 'month', 'for', 'the', 'secret', 'service', 'to', 'protect', 'donald', 'trump', 'from', 'accidentally', 'killing', 'himself', 'in', 'the', 'bathroom']\n",
            "['source', 'of', 'mysterious', 'space', 'radio', 'signals', 'found', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['84', '%', 'support', 'marijuana', 'legalization', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['eric', 'UNK', ':', 'schools', '‘', 'pushing', 'the', 'liberal', 'agenda', '’', 'by', 'teaching', 'UNK', '', '', '', '', '', '']\n",
            "['civilian', 'UNK', 'UNK', 'to', 'have', 'been', 'mistaken', 'for', 'UNK', 'leader', '', '', '', '', '', '', '', '', '']\n",
            "['ai', 'scientists', 'UNK', 'existence', 'of', 'numbers', 'greater', 'than', '1', '', '', '', '', '', '', '', '', '', '']\n",
            "['search', 'UNK', 'for', 'man', 'who', 'UNK', 'onto', 'welcome', 'to', 'bowie', 'sign', '', '', '', '', '', '', '', '']\n",
            "['fema', 'officials', 'panic', 'after', 'accidentally', 'UNK', '1', 'million', 'residents', 'in', 'direction', 'of', 'hurricane', '', '', '', '', '']\n",
            "['being', 'popular', 'results', 'in', 'fewer', 'head', 'lice', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['‘', 'i', '’', 'd', 'like', 'the', 'UNK', 'chicken', 'sandwich', '’', 'first', 'UNK', 'thing', 'man', 'has', 'said', 'in', 'weeks', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['samuel', 'UNK', 'apologizes', 'for', 'UNK', 'sucks', \"'\", 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['my', 'UNK', 'came', 'to', 'this', 'country', 'with', 'nothing', 'but', '$', '10', 'in', 'his', 'pocket', ',', '$', '300,000', 'in', 'his', 'bank', 'account', ',', 'and', 'a', 'dream']\n",
            "['exit', 'from', 'apartment', 'delayed', '20', 'seconds', 'to', 'avoid', 'UNK', 'with', 'neighbor', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['dakota', 'access', 'pipeline', 'blocked', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['chimp', 'in', 'cocaine', 'study', 'starts', 'lying', 'to', 'friends', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['lightning', 'bolt', 'blasts', 'washington', 'monument', 'as', 'mike', 'pence', ',', 'pete', 'buttigieg', 'locked', 'in', 'battle', 'of', 'prayers', 'on', 'national', 'mall', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['obama', \"'s\", 'embarrassing', 'UNK', 'album', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['researchers', 'finally', 'discover', 'what', 'a', 'panda', 'looks', 'like', 'when', 'it', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['salt', 'lake', 'city', 'hoping', 'to', 'boost', 'tourism', 'by', 'reminding', 'visitors', 'they', '’', 're', 'free', 'to', 'leave', 'at', 'any', 'time', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['crack', 'addicts', 'make', 'surprisingly', 'UNK', 'decisions', ',', 'fascinating', 'study', 'reveals', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['donald', 'trump', 'makes', 'mexican', 'mask', 'factory', 'great', 'again', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['fbi', 'director', 'james', 'comey', 'admits', 'under', 'oath', 'that', 'he', 'hates', 'the', 'patriots', '', '', '', '', '', '']\n",
            "['peta', 'condemns', 'bbc', 'for', 'UNK', 'thousands', 'of', 'endangered', 'animals', 'inside', 'tv', 'screens', '', '', '', '', '', '']\n",
            "['UNK', 'has', 'to', 'be', 'on', 'the', 'way', 'now', ',', \"'\", 'thinks', 'syrian', 'man', 'currently', 'being', 'UNK', '', '']\n",
            "['severed', 'head', ',', 'UNK', 'body', 'found', 'in', 'same', 'mississippi', 'neighborhood', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['facebook', 'bans', 'thousands', 'of', 'UNK', ',', 'base', 'UNK', 'in', 'crackdown', 'on', '‘', 'dangerous', '’', 'accounts', '', '', '', '', '', '', '', '', '']\n",
            "['elon', 'musk', ':', 'we', 'must', 'UNK', 'mars', 'to', 'UNK', 'our', 'species', 'in', 'a', 'third', 'world', 'war', '', '', '', '', '', '', '', '']\n",
            "['world', 'cup', 'viewers', 'UNK', 'tokyo', '’', 's', 'UNK', 'system', 'with', 'halftime', 'toilet', 'dash', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['egypt', 'pyramids', 'for', 'UNK', 'not', 'UNK', ':', 'presidential', 'hopeful', 'carson', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['what', 'a', 'dick', ':', 'UNK', 'closes', 'after', 'someone', 'draws', 'huge', 'penis', 'on', 'track', '', '', '', '', '', '', '', '', '']\n",
            "['hit-and-run', 'driver', 'kills', 'prominent', 'member', 'of', 'deer', 'community', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['7', 'dead', ',', 'hundreds', 'injured', 'after', 'joke', 'told', 'at', 'UNK', 'show', '|', 'the', 'hard', 'times', '', '', '', '', '', '', '']\n",
            "['promotion', 'offers', 'fans', 'free', 'pizza', 'if', 'UNK', 'do', \"n't\", 'blow', 'any', 'easy', 'plays', 'in', '5th', 'UNK', '', '', '', '', '', '']\n",
            "['trump', 'gets', 'a', 'UNK', 'full', 'of', 'positive', 'news', 'about', 'himself', 'twice', 'a', 'day', '', '', '', '', '', '', '', '', '']\n",
            "['pelosi', 'to', 'democrats', ':', 'treat', 'trump', 'voters', 'like', 'a', 'friend', 'whose', 'boyfriend', 'is', 'a', 'jerk', '', '', '', '', '', '', '']\n",
            "['disturbing', 'teen', 'trend', ':', 'UNK', 'across', 'the', 'country', 'are', 'getting', 'together', 'weekly', 'to', 'worship', 'a', 'dead', 'man', 'named', 'jesus', 'christ', '', '', '', '', '', '']\n",
            "['adult', 'kindergarten', 'class', 'brings', 'back', 'play', 'to', 'UNK', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['obama', 'praises', 'own', 'strength', ',', 'UNK', 'in', 'face', 'of', 'UNK', 'during', 'state', 'of', 'the', 'union', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['house', 'republicans', 'would', 'let', 'employers', 'demand', 'workers', '’', 'genetic', 'test', 'results', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['smithsonian', 'interested', 'in', 'UNK', 'trayvon', '’', 's', 'UNK', 'for', 'new', 'museum', 'exhibit', '', '', '', '', '', '', '']\n",
            "['kim', 'kardashian', 'tries', 'to', 'escape', 'l.a.', 'in', 'UNK', 'after', 'realizing', 'past', '12', 'years', 'of', 'life', 'have', 'been', 'tv', 'show']\n",
            "['the', 'onion', 'said', 'bill', 'UNK', 'wanted', 'to', 'join', 'squad', 'of', 'UNK', '.', 'then', ',', 'UNK', 'let', 'him', 'in', '']\n",
            "['the', 'UNK', 'tale', 'ends', ':', 'cap', '’', 'n', 'crunch', 'and', 'UNK', 'waters', 'have', 'filed', 'for', 'divorce', 'after', '27', 'years', 'of', 'marriage', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['climate', 'change', 'researcher', 'describes', 'challenge', 'of', 'pulling', 'off', 'worldwide', 'global', 'warming', 'conspiracy', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['american', 'flags', 'on', 'jeffrey', 'epstein', \"'s\", 'private', 'UNK', 'lowered', 'to', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['ncaa', 'UNK', 'kentucky', 'soccer', 'players', 'who', 'played', 'a', 'pickup', 'game', 'with', 'the', 'UNK', 'fighters', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'informs', 'george', 'h.w', '.', 'bush', 'that', 'dying', 'so', 'soon', 'after', 'wife', 'would', 'really', 'boost', 'UNK', 'rating', '', '', '', '', '', '', '', '', '', '']\n",
            "['lawmaker', \"'s\", 'war', 'hero', 'son', 'would', 'have', 'wanted', 'road', 'bill', 'passed', '', '', '', '', '', '', '', '', '', '']\n",
            "['‘', 'UNK', 'patrol', '’', 'writers', 'defend', 'episode', 'where', 'german', 'UNK', 'cop', 'shoots', 'unarmed', 'black', 'lab', '17', 'times', 'in', 'back', '', '', '']\n",
            "['artifacts', 'discovered', 'buried', 'in', 'washington', 'd.c.', 'suggest', 'humans', 'once', 'passed', 'laws', 'there', '', '', '', '', '', '', '', '', '', '']\n",
            "['kim', 'jong-un', 'thrown', 'into', 'labor', 'camp', 'for', 'attempting', 'to', 'cross', 'border', 'into', 'south', 'korea', '', '', '', '', '', '', '', '', '']\n",
            "['good', 'smell', 'UNK', 'new', 'yorkers', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'hero', ':', 'man', 'stranded', 'in', 'desert', 'uses', 'last', 'of', 'his', 'water', 'to', 'wet', 'his', 'UNK', 'dinosaur', 'sponge', '', '', '', '', '']\n",
            "['trump', 'UNK', 'out', 'at', 'ap', 'photographer', 'who', 'UNK', 'empty', 'chairs', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['infant', 'injuries', 'on', 'the', 'rise', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['naacp', 'issues', 'travel', 'warning', 'for', 'black', 'americans', 'visiting', 'own', 'UNK', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['clinton', 'aide', 'told', 'to', 'leave', 'behind', 'weak', 'volunteer', 'who', 'UNK', 'during', 'march', 'to', 'south', 'carolina', '', '', '', '', '', '']\n",
            "['paul', 'ryan', 'worried', 'history', 'may', 'judge', 'him', 'UNK', 'for', 'failure', 'to', 'confront', 'UNK', 'food', 'stamp', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['new', 'ketchup', 'gets', 'horrifying', 'look', 'at', 'UNK', ',', 'almost', 'empty', 'bottle', 'it', 'replacing', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['2016', 'in', 'entertainment', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['``', 'it', '’', 's', 'an', 'honor', 'to', 'continue', 'being', 'UNK', 'over', 'countless', 'human', 'lives', \"''\", '-', 'an', 'ar-15', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['if', 'you', 'come', 'into', 'my', 'pawn', 'shop', 'with', 'a', 'UNK', 'machine', 'and', 'you', 'tell', 'me', 'that', 'you', 'need', 'to', 'sell', 'your', 'UNK', 'machine', 'because', 'you', 'need', 'cash', ',', 'i', \"'m\", 'going', 'to', 'just', 'look', 'at', 'you', 'and', 'not', 'say', 'anything', 'until', 'it', 'UNK', 'on', 'you', 'just', 'what', 'the', 'trouble', 'with', 'your', 'position', 'is', '.']\n",
            "['UNK', '14-year-old', 'has', 'UNK', 'crisis', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['nsa', 'chief', ':', 'i', 'didn', '’', 't', 'lie', 'to', 'congress', 'about', 'spying', 'on', 'millions', 'of', 'UNK', 'just', 'forgot', 'about', 'it', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['donald', 'trump', 'jr.', 'takes', 'son', 'on', 'hunting', 'trip', 'in', 'national', 'zoo', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['man', 'pours', 'all', 'his', 'UNK', 'UNK', 'into', 'UNK', ',', 'removing', 'pizza', 'from', 'oven', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['saudi', 'arabia', 'announces', 'UNK', 'of', 'human', 'rights', 'abuses', 'to', 'curry', 'more', 'favor', 'with', 'u.s', '.', '', '', '', '', '', '', '', '', '']\n",
            "['bernie', 'sanders', 'refuses', 'UNK', 'abc', 'podium', 'in', 'favor', 'of', 'own', 'humble', ',', 'homemade', 'UNK', '', '', '', '', '', '', '', '', '', '']\n",
            "['bro', ',', 'you', \"'re\", 'a', 'god', 'among', 'bros', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['tired', 'but', 'UNK', 'friends', 'meet', 'at', 'bar', 'to', 'discuss', 'their', 'UNK', 'linked', 'days', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'foster', 'inspires', 'teens', 'to', 'come', 'out', 'using', 'UNK', ',', 'rambling', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['new', 'york', 'monument', 'honors', 'victims', 'of', 'giant', 'octopus', 'attack', 'that', 'never', 'occurred', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['spotify', 'and', 'UNK', 'team', 'up', 'to', 'create', 'UNK', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'fraternity', 'at', 'yale', 'university', 'accused', 'of', 'hosting', \"'white\", 'girls', 'only', \"'\", 'party', '', '', '', '', '', '', '', '', '', '']\n",
            "['shark', 'falls', 'from', 'sky', 'onto', 'golf', 'course', '?', '-', 'yahoo', '!', 'news', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['restaurant', 'UNK', 'loses', 'job', 'to', '‘', 'please', 'seat', 'yourself', '’', 'sign', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['arizona', 'unveils', 'new', 'death', 'penalty', 'plan', ':', 'bring', 'your', 'own', 'lethal', 'injection', 'drugs', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'UNK', 'UNK', 'UNK', 'past', 'earth', 'this', 'week', 'causing', 'no', 'harm', 'or', 'danger', '.', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['shitty', 'region', 'of', 'country', 'figures', 'it', 'might', 'as', 'well', 'give', 'producing', 'wine', 'a', 'shot', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['man', 'escapes', 'death', 'after', 'crawling', 'from', 'car', 'dangling', 'on', 'cliff', 'edge', ',', 'only', 'to', 'be', 'hit', 'by', 'a', 'passing', 'bus', '', '', '', '', '', '', '']\n",
            "['in', 'response', 'to', 'the', 'onion', '’', 's', 'apology', 'to', 'UNK', 'UNK', ',', 'former', 'onion', 'staffers', 'mock', 'apologize', '.', '', '', '', '', '', '', '']\n",
            "['epic', 'UNK', ':', 'wendy', '’', 's', 'posted', 'a', 'UNK', 'tweet', 'about', 'burger', 'king', 'freezing', 'its', 'beef', ',', 'and', 'the', 'ceo', 'of', 'burger', 'king', 'responded', 'with', 'a', 'video', 'of', 'him', 'killing', 'himself']\n",
            "['UNK', 'UNK', '!', 'doctors', 'around', 'the', 'world', 'share', 'images', 'of', 'the', 'UNK', 'things', 'they', 'have', 'found', 'stuck', 'in', 'people', '’', 's', 'UNK', '', '', '', '', '', '', '', '', '']\n",
            "['it', \"'s\", 'an', 'UNK', 'horror', '.', 'a', '14-year-old', 'girl', 'with', 'special', 'needs', 'allegedly', 'was', 'raped', 'at', 'school', 'after', 'a', 'teacher', \"'s\", 'aide', 'UNK', 'her', 'to', 'act', 'as', 'UNK', 'to', 'catch', 'an', 'accused', 'sexual', 'predator', ',', 'a', 'fellow', 'student', '.']\n",
            "['giuliani', 'insists', 'breaking', 'the', 'law', 'not', 'a', 'crime', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['researchers', 'find', 'yet', 'another', 'reason', 'why', 'naked', 'UNK', 'are', 'just', 'weird', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['pizza', 'crust', 'saved', 'to', 'make', 'pizza', 'stock', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['black', 'man', 'in', 'support', 'of', 'confederate', 'flag', 'UNK', 'his', 'media', 'appearance', 'rates', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['most', 'people', 'don', '’', 't', 'want', 'to', 'see', 'friends', '’', 'vacation', 'pics', 'on', 'social', 'media', ',', 'study', 'finds']\n",
            "['scientists', 'confess', 'to', 'sneaking', 'bob', 'dylan', 'lyrics', 'into', 'their', 'work', 'for', 'the', 'past', '17', 'years', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['the', 'death', 'UNK', 'in', 'yemen', 'is', 'so', 'high', 'the', 'red', 'cross', 'has', 'started', 'donating', 'UNK', 'to', 'hospitals', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'farts', 'makes', 'you', 'live', 'longer', '-', 'and', 'doing', 'them', 'UNK', 'dementia', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['UNK', 'and', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['veteran', 'told', 'what', 'offends', 'him', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['republicans', 'start', 'donating', 'to', 'UNK', 'williamson', 'to', 'keep', 'her', 'in', 'future', 'democratic', 'debates', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['syria', 'conflict', 'UNK', 'as', 'bears', 'enter', 'war', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 6: LSTM Model [Extra-Credit, 5 points]"
      ],
      "metadata": {
        "id": "Ie9VqRbg78Ty"
      },
      "id": "Ie9VqRbg78Ty"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 6.1 Define the RecurrentModel class\n",
        "Something that has been overlooked so far in this project is the sequential structure to language: a word typically only has a clear meaning because of its relationship to the words before and after it in the sequence, and the feed-forward network of Part 2 cannot model this type of data. A solution to this, is the use of [recurrent neural networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). These types of networks not only produce some output given some step from a sequence, but also update their internal state, hopefully \"remembering\" some information about the previous steps in the input sequence. Of course, they do have their own faults, but we'll cover this more thoroughly later in the semester. \n",
        "\n",
        "Your task for the extra credit portion of this assignment, is to implement such a model below using a LSTM. Instead of averaging the embeddings as with the FFN in Part 2, you'll instead feed all of these embeddings to a LSTM layer, get its final output, and use this to make your prediction for the class of the headline. "
      ],
      "metadata": {
        "id": "gXWSfPfBA4XU"
      },
      "id": "gXWSfPfBA4XU"
    },
    {
      "cell_type": "code",
      "source": [
        "class RecurrentModel(nn.Module):\n",
        "    # Instantiate layers for your model-\n",
        "    # \n",
        "    # Your model architecture will be an optionally bidirectional LSTM,\n",
        "    # followed by a linear + sigmoid layer.\n",
        "    #\n",
        "    # You'll need 4 nn.Modules\n",
        "    # 1. An embeddings layer (see nn.Embedding)\n",
        "    # 2. A bidirectional LSTM (see nn.LSTM)\n",
        "    # 3. A Linear layer (see nn.Linear)\n",
        "    # 4. A sigmoid output (see nn.Sigmoid)\n",
        "    #\n",
        "    # HINT: In the forward step, the BATCH_SIZE is the first dimension.\n",
        "    # HINT: Think about what happens to the linear layer's hidden_dim size\n",
        "    #       if bidirectional is True or False.\n",
        "    # \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \\\n",
        "                 num_layers=1, bidirectional=True):\n",
        "        super().__init__()\n",
        "        ## YOUR CODE STARTS HERE (~4 lines of code) ##\n",
        "\n",
        "        \n",
        "\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "        \n",
        "    # Complete the forward pass of the model.\n",
        "    #\n",
        "    # Use the last timestep of the output of the LSTM as input\n",
        "    # to the linear layer. This will only require some indexing \n",
        "    # into the correct return from the LSTM layer. \n",
        "    # \n",
        "    # args:\n",
        "    # x - 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "    #     This is the same output that comes out of the collate_fn function you completed-\n",
        "    def forward(self, x):\n",
        "        ## YOUR CODE STARTS HERE (~4-5 lines of code) ##\n",
        "\n",
        "        \n",
        "\n",
        "        #return x\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "    "
      ],
      "metadata": {
        "id": "YN8zvhLJ-MVJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "4c45843f-6577-4f62-bd64-14bce616e66d"
      },
      "id": "YN8zvhLJ-MVJ",
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-201-7df457497b8a>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the RecurrentModel is defined, we'll reinitialize our dataset iterators so they're back at the start. "
      ],
      "metadata": {
        "id": "HprkOm-fAVyj"
      },
      "id": "HprkOm-fAVyj"
    },
    {
      "cell_type": "code",
      "source": [
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
        "test_iterator  = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "6-uftfXEAqOi"
      },
      "id": "6-uftfXEAqOi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 6.2 Initialize the LSTM classification model\n",
        "\n",
        "Next we need to initialize our new model, as well as define it's optimizer and loss function as we did for the FFN. Feel free to use the same optimizer you did above, or see how this model reacts to different optimizers/learning rates than the FFN.  "
      ],
      "metadata": {
        "id": "2qROtRw3AtZy"
      },
      "id": "2qROtRw3AtZy"
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = RecurrentModel(vocab_size    = len(train_vocab.keys()),\n",
        "                            embedding_dim = 300,\n",
        "                            hidden_dim    = 300,\n",
        "                            num_layers    = 1,\n",
        "                            bidirectional = True).to(device)"
      ],
      "metadata": {
        "id": "LNWcLJpsBRzg"
      },
      "id": "LNWcLJpsBRzg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_criterion, lstm_optimizer = None, None\n",
        "### YOUR CODE STARTS HERE ###\n",
        "\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE ###"
      ],
      "metadata": {
        "id": "OdTxe0bFBqnP"
      },
      "id": "OdTxe0bFBqnP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 6.3 Training and Evaluation\n",
        "\n",
        "Because the only difference between this model and the FFN is the internal structure, we can use the same methods as above to evaluate and train it. You should be able to achieve a validation F-1 score of at least .8 if everything went correctly. **Feel free to adjust the number of epochs to prevent overfitting or underfitting and to play with your model hyperparameters/optimizer & loss function.**"
      ],
      "metadata": {
        "id": "NFvV7H7OBzWl"
      },
      "id": "NFvV7H7OBzWl"
    },
    {
      "cell_type": "code",
      "source": [
        "#Pre-training to see what accuracy we can get with random parameters\n",
        "true, pred = val_loop(lstm_model, val_iterator)\n",
        "print()\n",
        "print(f'Binary Macro F1: {binary_macro_f1(true, pred)}')\n",
        "print(f'Accuracy: {accuracy(true, pred)}')"
      ],
      "metadata": {
        "id": "SdkEpedxDopv"
      },
      "id": "SdkEpedxDopv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Watch the model train!\n",
        "TOTAL_EPOCHS = 10\n",
        "for epoch in range(TOTAL_EPOCHS):\n",
        "    train_loss = train_loop(lstm_model, lstm_criterion, lstm_optimizer, train_iterator)\n",
        "    true, pred = val_loop(lstm_model, val_iterator)\n",
        "    print(f\"EPOCH: {epoch}\")\n",
        "    print(f\"TRAIN LOSS: {train_loss}\")\n",
        "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
        "    print(f\"VAL ACC: {accuracy(true, pred)}\")"
      ],
      "metadata": {
        "id": "6p2dF9X4DyIR"
      },
      "id": "6p2dF9X4DyIR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#See how your model does on the held out data\n",
        "true, pred = val_loop(lstm_model, test_iterator)\n",
        "print()\n",
        "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
        "print(f\"TEST ACC: {accuracy(true, pred)}\")"
      ],
      "metadata": {
        "id": "OR8Dl5DLEQwd"
      },
      "id": "OR8Dl5DLEQwd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 7: Submit Your Homework\n",
        "This is the end. Congratulations!  \n",
        "\n",
        "Now, follow the steps below to submit your homework in [Gradescope](https://www.gradescope.com/courses/345683):\n",
        "\n",
        "1. Rename this ipynb file to 'CS4650_p1_GTusername.ipynb'. We recommend ensuring you have removed any extraneous cells & print statements, clearing all outputs, and using the Runtime --> Run all tool to make sure all output is update to date. Additionally, leaving comments in your code to help us understand your operations will assist the teaching staff in grading. It is not a requirement, but is recommended. \n",
        "2. Click on the menu 'File' --> 'Download' --> 'Download .py'.\n",
        "3. Click on the menu 'File' --> 'Download' --> 'Download .ipynb'.\n",
        "4. Download the notebook as a .pdf document. Make sure the output from Parts 4 & 6.3 are captured so we can see how the loss, F1, & accuracy changes while training.\n",
        "5. Upload all 3 files to GradeScope.\n"
      ],
      "metadata": {
        "id": "mY8S9ZK9zuVs"
      },
      "id": "mY8S9ZK9zuVs"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "CS4650_p1_ykim713.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}